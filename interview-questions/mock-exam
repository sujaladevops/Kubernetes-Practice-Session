1. Take a backup of the etcd cluster and save it to /opt/etcd-backup.db

Ans: https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster
#ETCDCTL_API=3 etcdctl --endpoints $ENDPOINT snapshot save snapshot.db

ETCDCTL_API=3 etcdctl snapshot save -h 
cat /etckubernetes/manifest/etcd.yaml | grep file
For endpoint: vi /etckubernetes/manifest/etcd.yaml
ListenUrl=https://127.0.0.1:2379 (copy it)

ETCDCTL_API=3 etcdctl --endpoints 127.0.0.1:2379 snapshot save /opt/etcd-backup.db \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  >>snapshot saved at /opt/etcd-backup.db

---------------------------------------------------------------------------------------------------------------
2. Create a Pod called redis-storage with image redis:alpine with a volume of type emptyDir that lasts for the life of the pods.
Pod named 'redis-storage' created
Pod 'redis-storage' uses Volume type of emprtyDir
Pod 'redis-storage' uses volumeMount with mountpath=/data/redis
https://kubernetes.io/docs/concepts/storage/volumes/#emptydir-configuration-example
Ans:
k run redis-storage --image=redis:alpine --dry-run=client -o yaml > redis-storage.yaml
In yaml file, add volumes and volumeMounts
apiVersion: v1
kind: Pod
metadata:
  name: redis-storage
spec:
  containers:
  - name: redis-storage
    image: redis:alpine
    volumeMounts:
    - name: redis-storage-volume
      mountPath: /data/redis
  volumes:
  - name: redis-storage-volume
    emptyDir: {}

k apply -f redis-storage.yaml

---------------------------------------------------------------------------------------------------------------

3. Create a new pod called super-user-pod with image busybox:1.28. Allow the pod to be able to set system_time.
The container should sleep for 4800 seconds
Pod: super-user-pod
container image: busybox:1.28
SYS_TIME capabilities for the container?
Ans:
https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container
k run super-user-pod --image=busybox:1.28 --dry-run=client -o yaml >suerp-user.yaml
apiVersion: v1
kind: Pod
metadata:
  name: super-user-pod
spec:
  containers:
  - name: busybox-container
    image: busybox:1.28
    command: ["/bin/sh", "-c", "sleep 4800"]
    securityContext:
      capabilities:
        add:
        - SYS_TIME

---------------------------------------------------------------------------------------------------------------
4. A pod definition create at /root/CKA/user-pv.yaml.
Make use of this manifest file and mount the persistent volume called pv-1. 
Ensure the pod is running and the PV is bound.
mountpath: /data
persistentVolumeClaimName: my-pvc
Verify the below
persistentVolume Claim configured correctly?
Pod using the correct mountpath?
Pod using the persistent volume claim?

Ans:
cat /root/CKA/user-pv.yaml
apiVersion: v1
kind: pod
metadata:
  creationTimestamp: null
  labels:
    run: use-pv
  name: use-pv
spec:
  containers:
  - image:nginx
    name: use-pv
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

k get pv
NAME   Capacity  AccessModes
pv-1   10Mi      RWO

k get pvc
no resources 

vi pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Mi
 
k apply -f pvc.yaml
k get pvc
my-pvc  Bound   pv-1   10Mi   RWO

Edit the given pod file, using https://kubernetes.io/docs/concepts/storage/persistent-volumes/#claims-as-volumes
vi /root/CKA/user-pv.yaml
apiVersion: v1
kind: pod
metadata:
  creationTimestamp: null
  labels:
    run: use-pv
  name: use-pv
spec:
  containers:
  - image:nginx
    name: use-pv
    resources: {}
    volumeMounts:
      - mountPath: "/data"
        name: mypd
  dnsPolicy: ClusterFirst
  restartPolicy: Always
    volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: my-pvc
status: {}

k create -f /root/CKA/user-pv.yaml
k get pods
k get pods --watch
k describe pod use-pv (check pv, pvc, mountpath)
---------------------------------------------------------------------------------------------------------------
5. Create a new deployment called nginx-deploy, with image nginx:1.16 and 1 replica. 
Next upgrade the deployment to version 1.17 using rolling update.
Deployment: nginx-deployment, Image: nginx:1.16
Image: nginx:1.16
Task: Upgrade the version of the deployment to 1.17
Task: Record the changes for the image upgrade.
Ans:
k create deployment nginx-deploy --image=nginx:1.16 --replicas=1
k get deploy
k describe deployment nginx-deploy
k set image deployment/nginx-deploy nginx(this is container, so check it in describe again)=nginx:1.17
---------------------------------------------------------------------------------------------------------------

6. Create a new cluster called John. Grant him access to the cluster. 
John should have permissions to create, list, get, updaet and delete pods in the development namespace. 
The private key exists in the location: /root/CKA/john.key and csr at /root/CKA/john.csr
Important Note: As of kubernetes 1.19, the CertificateSigningRequest object expects a signerName.
Ans:
https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#create-certificatessigningrequest
CSR: 'john-developer', Status:Approved
Role Name:'developer';Namespace:'development';Resource:'Pods'
Access: User 'john' has appropriate permissions

Ans:
cat john.csr
vim john-csr.yaml

apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: john-developer
spec:
  request: ksdhfgruhgjgbhjbhdfgbdgjbdhfgbfhdb
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - client auth

cat john.csr | base64 | tr -d "\n"
ksdhfgruhgjgbhjbhdfgbdgjbdhfgbfhdb

vi john-csr.yaml
copy paste the above generated cert in the yaml file
k create -f john-csr.yaml 
k get csr
>>john-developer   pending(condition)
k certificate approve john-developer
k create clusterrole foo --verb=get,list,watch --resource=pods,pods/status (from help/cheatsheet)

k create clusterrole developer --verb=create,get,list,update,delete --resource=pods -n development

k get role -n development
k describe role -n development
k auth can-i get pods --namespace=development --as john
no
k auth can-i create pods --namespace=development --as john
no

k create rolebinding john-developer --role=developer --user=john -n development
k auth can-i get pods --namespace=development --as john
yes
k auth can-i create pods --namespace=development --as john
yes

---------------------------------------------------------------------------------------------------------------

7. Create a nginx pod called nginx-resolver using image nginx, expose it internally with a servie called nginx-resolver-service.
Test you are able to look up the service and pod names from within the cluster. Use the image busybox:1.28 to create a pod dns lookup. 
Record results in /root/CKA/nginx.svc and /root/CKA/nginx.pod for service and pod name resolutions respectively.
pod: nginx-resolver created
service DNS resolution recorded correctly
pod DNS resolution recorded correctly
Ans: 
k run nginx-resolver --image=nginx
k expose pod nginx-resolver --name=nginx-resolver-service --port=80
k describe svc nginx-resolver-service
k run busybox --image=busybox:1.28 -- sleep 4000
k get pods
k exec buysbox -- nslookup nginx-resolver-service > /root/CKA/nginx.svc
https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pods

k get pods -o wide (get the IP)
k exec buysbox -- nslookup 10-50-192-4.default.pod.cluster.local > /root/CKA/nginx.svc
---------------------------------------------------------------------------------------------------------------

8. Create a static pod on node01 called nginx-critical with image nginx and make sure that it is recreated/restarted automatically
in case of a failure.
Use /etc/kubernetes/manifests as the Static Pod path for example.
Ans:
k get nodes -o wide
ssh 10.7.80.3
ls /etc/kubernetes/manifests
controlplane$ k run nginx-critical --image=nginx --restart=Always --dry-run=client -o yaml
copy the yaml file paste it in node01
node01$ cat > /etc/kubernetes/manifests/nginx-critical.yaml

---------------------------------------------------------------------------------------------------------------
1. Create a new service accoutn with the name pvviewer. Grant this service account access to list all PersistentVolumes in the cluster
by creating an appropriate cluster role called pvviewer-role and ClusterRoleBinding called pvviewer-role-binding.
Next, create a pod called pvviewer with the image:redis and serviceAccount:pvviewer in the default namespace.
Ans:
k create sa pvviewer
k get sa
k clusterrole --help
k clusterrole pvviewer-role --verb=list --resources=persistentvolumes
k create clusterrolebinding --help
k create clusterrolebinding pvviewer-role-binding --clusterrole=pvviewer-role --serviceaccount=default:pvviewer

k describe clusterrolebinding pvviewer-role-binding
k run pvviewer --image=redis --dry-run=client -o yaml > pvviewer.yaml

k apply -f pvviewer.yaml

---------------------------------------------------------------------------------------------------------------
2. List the InternalIP of all nodes of the cluster. Save the result to a file /root/CKA/node_ips.
Answer should be in the format InternalIP of clusterplane <space> InternalIP of node01 
Ans:
k get nodes -o wide
k get nodes -o json | jq | grep -i internalip -B 100  --> itesms.status.addresses.type
OR
k get nodes -o json | jq -c 'paths' | grep type | grep -v condition
k get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}' | jq
k get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}'
k get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}'

k get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}' > /root/CKA/node_ips
---------------------------------------------------------------------------------------------------------------
3. Create a pod called multi-pod with tqo containers/
Container1 name alpha, image: nginx
container2 name: beta, image:busybox, command: sleep 4000
Ans:
k run --image=nginx --dryn-client=client -o yaml >multi-pod.yaml
containers:
-  image: nginx
   name: alpha
    env:
     - name: "name"
       value: "alpha"
-  image: busybox
   name: beta
   command:
     - sleep:
     - "4000"
    env:
     - name: "name"
       value: "beta"
---------------------------------------------------------------------------------------------------------------
4. Create a pod called non-root-pod, image:redis:alpine
runAsUser: 1000
fsGroup: 2000
Ans: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
k run non-root-pod --image=redis:alpine --dry-run=client -o yaml > non-root-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: non-root-pod
  name: non-root-pod
spec:
  securityContext:
    runAsUser: 1000
    fsGroup: 2000
  volumes:
  - name: sec-ctx-vol
    emptyDir: {}
  containers:
  - name: non-root-pod
    image: redis:alpine
    resourvces: {}
  dnsPolicy: ClusterFirst
  restartpolicy: Always
status: {}

k apply -f non-root-pod.yaml
---------------------------------------------------------------------------------------------------------------
5. We have deployed a new pod called np-test-1 and a service called no-test-service. Incoming connectopns to this service are not working Troubleshoot and fix it.
Create NetworkPolicy by the name ingress-to-nptest that allows incoming connections to the service over port 80.
Ans:
k get pod
k get svc
k run curl --image=alpine/curl --rm -it -- sh
# curl np-test-service
controlplane$ k get pod --show-labels
np-test-1  run=np-test-1

vim network-policy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ingress-to-nptest
spec:
  podSelector:
    matchLabels:
      run: np-test-1
  ingress:
  - ports:
    - protocol: TCP
      port: 80
k apply -f network-policy.yaml

# curl np-test-service
---------------------------------------------------------------------------------------------------------------

6. Taint the worker node node01 to be Unschedulable. Once done, create a pod called dev-redis, image redis:alpine to ensure workloads are not
scheduled to this worker node. Finally, create a new pod called prod-redis and image: redis:alpine with toleration to be scheduled on node01.
key: env_type, value:production, operator:Equal and effect: NoSchedule
Key=env_type
Value=production
Effect=NoSchedule
prod 'dev-redis' (no tolerations) is not scheduled on node01?
Create a pod 'prod-redis' to run on node01
Ans:
k taint nodes 
kubectl taint nodes node01 key=value:NoSchedule
kubectl taint nodes node01 env_type=production:NoSchedule
k desribe node node01
k run dev-redis --image=redis:alpine
k get pods -o wide

k run prod-redis --image=redis:alpine --dry-run=client -o yaml > prod-redis.yaml
Edit the file based on this: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/#concepts
apiVersion: v1
kind: Pod
metadata:
  name: prod-redis
spec:
  containers:
  - name: redis-container
    image: redis:alpine
  tolerations:
  - key: env_type
    operator: Equal
    value: production
    effect: NoSchedule

k apply -f prod-redis.yaml

---------------------------------------------------------------------------------------------------------------
7. Create a pod called hr-pod in hr namespace belonging to the production environment and frontend tier.
Image: redis:alpine
Use appropriate labels and create all the required objects if it doesn't not exist in the system already.
hr-pod labeled with environment production?
hr-pod labeled with the frontend?
Ans:
k get ns 
k create ns hr
k run hr-pod -n hr --image=redis:alpine --labels="environment=production,tier=frontend"
pod/hr-pod created
k get pods -n hr
---------------------------------------------------------------------------------------------------------------
8. A kubeconfig file called super.kubeconfig has been created under /root/CKA. There is something wrong with the configuration. 
Troubleshoot and fix it.
Ans:
k get nodes --kubeconfig /root/CKA/super.kubeconfig
error: "https://controlplane:9999/api?timeout=32s"

cat .kube/config
default port is 6443
vi /root/CKA/super.kubeconfig
update the code with 6443 port

k get nodes --kubeconfig /root/CKA/super.kubeconfig
---------------------------------------------------------------------------------------------------------------
9. We have created a new deployment called nginx-deploy. Scale the deployment to 3 replicas.
Has the replicas increased? Troubleshoot the issue and fix it.
Ans:
k get deployments
k scale deployment nginx-ddeploy --replicas=3
k describe deployment
3 desired but Message says, scaled up replica set nginx-deply-678 to 1

So, there's an issue here only taking replica as 1

S0, need to check with controller manager.
k get pods -n kube-system
kube-controller-manager-controlplane   ImagePullBackOff

cd /etc/kubernetes/manifests
vim kube-controller-manager.yaml
spelling mistakes: controller

k get pods -n kube-system

k get deployments