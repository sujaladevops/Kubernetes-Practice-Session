1.Create a new ClusterRole named deployment-clusterrole, which only allows to create the following resource types:
✑ Deployment
✑ Stateful Set
✑ DaemonSet
Create a new ServiceAccount named cicd-token in the existing namespace app-team1.
Bind the new ClusterRole deployment-clusterrole to the new ServiceAccount cicd-token, limited to the namespace app-team1.
Ans:

kubectl config use-context k8s
kubectl create clusterrole deployment-clusterrole --verb=create --resource=deployments,statefulsets,daemonsets -n app-team1
kubectl create sa cicd-token --namespace app-team1
kubectl create clusterrolebinding deploy-b --clusterrole=deployment-clusterrole --serviceaccount=app-team1:cicd-token

2. Set the node named ek8s-node-0 as unavailable and reschedule all the pods running on it.
kubectl config use-context ek8s
k get nodes
NAME            STATUS        ROLES              AGE       VERSION
ek8s-master-0    READY    control-plane,master   67d       v1.23.1
ek8s-node-0      READY       <none>              67d       v1.23.1
ek8s-node-1      READY       <none>              67d       v1.23.1

kubectl cordon ek8s-node-0 (may or may not need it)
k drain ek8s-node0 --ignore-daemonsets
k drain ek8s-node0 --ignore-daemonsets --delete-emptydir-data/k drain ek8s-node-0 --delete-local-data --ignore-daemonsets --force
k get nodes
k get pods -o wide


3. Given an existing Kubernetes cluster running version 1.22.1, upgrade all of the Kubernetes control plane and node components on the master
node only to version 1.22.2.
Be sure to drain the master node before upgrading it and uncordon it after the upgrade.
You are also expected to upgrade kubelet and kubectl on the master node.
You can ssh to the master node using:
ssh k8s-master-0
You can assume elevated privileges on the master ndoe with the following command:
sudo -i
NOTE: Do not upgrade the worker nodes, etcd, the container manager, the CNI plugin, the DNS service or any other addons.

Ans:
k config use-context mk8s
k get nodes
mk8s-master-0     Ready      Control-plane, master     v1.22.1
mk8s-node-0       Ready      <none>                    v1.22.1
k drainmk8s-master-0 --ignore-daemonsets (cordoned)
k get nodes
ssh mk8s-master-0
sudo -i
root@mk8s-master-0:-apt install kubeadm=1.22.2-00 kubelet=1.22.2-00 kubectl=1.22.2-00
root@mk8s-master-0:-apt install kubeadm=1.22.2-00 kubelet=1.22.2-00 kubectl=1.22.2-00
root@mk8s-master-0:-kubeadm upgrade plan
root@mk8s-master-0:-kubeadm upgrade apply v1.22.2
root@mk8s-master-0:-systemctl restart kubelet
root@mk8s-master-0:-exit
student@node-1:-kubectl uncordon mk8s-master-0 (uncordoned)
k get nodes
mk8s-master-0     Ready      Control-plane, master     v1.22.2
mk8s-node-0       Ready      <none>                    v1.22.1


4.First, create a snapshot of the existing etcd instance running at https://127.0.0.1:2379, saving the snapshot to /var/lib/backup/etcd-snapshot.db.
Next, restore an existing, previous snapshot located at /var/lib/backup/etcd-snapshot-previous.db.
Ans:
vi /etc/kubernetes/manifest/etcd.yaml (will find the path for crt/key)
ETCDC TL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=<trusted-ca-file> --cert=<cert-file> --key=<key-file> \
  snapshot save <backup-file-location>

ETCDCTL_API=3 etcdctl --data-dir <data-dir-location> snapshot restore snapshot.db

5. Create a new NetworkPolicy named allow-port-from-namespace in the existing namespace fubar.
Ensure that the new NetworkPolicy allows Pods in namespace internal to connect to port 9000 of Pods in namespace fubar.
Further ensure that the new NetworkPolicy:
✑ does not allow access to Pods, which don't listen on port 9000
✑ does not allow access from Pods, which are not in namespace internal
vim policy.yaml
kubectl label ns my-app project=my-app
k describe ns my-app
k create -f policy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-port-from-namespace
  namespace: fubar
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: internal
    ports:
    - protocol: TCP
      port: 9000


6. Reconfigure the existing deployment front-end and add a port specification named http exposing port 80/tcp of the existing container nginx.
Create a new service named front-end-svc exposing the container port http.
Configure the new service to also expose the individual Pods via a NodePort on the nodes on which they are scheduled.

Ans:
k config use-context k8s
k get deployment.apps
k edit deployment.apps front-end
vi reconfigure-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: front-end
  labels:
    app: front-end
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: front-end
    spec:
      containers:
      - name: front-end
        image: nginx:1.14.2
        ports:
        - containerPort: 80
apiVersion: v1
kind: Service
metadata:
  name: front-end-svc
spec:
  selector:
    app: nginx
  ports:
  - name: http
    protocol: TCP
    port: 80
    targetPort: 80
  type: NodePort

7. Scale the deployment to 3 Pods.
k get deployments
k scale deployment_name --replicas=3
k get deployments 
k get pods

8. Schedule a pod as follows:
✑ Name: nginx-kusc00401
✑ Image: nginx
✑ Node selector: disk=ssd

Ans:
k run nginx-kusc00401 --image=nginx --dry-run=client -o yaml
k run nginx-kusc00401 --image=nginx --dry-run=client -o yaml > 8.yaml
8.yaml
apiVersion: v1
kind: Pod
metadata:
labels:
run: nginx-kusc00401
name: nginx-kusc00401
spec:
containers:
- image: nginx
  name: nginx
nodeSelector:
disk: ssd

9. Check to see how many nodes are ready (not including nodes tainted NoSchedule) and write the number to /opt/KUSC00402/kusc00402.txt.
To check how many are tainted:
kubectl get nodes --no-headers -o custom-columns=NAME:.metadata.name,STATUS:.status.conditions[?(@.type=="Ready")].status | grep -w True | wc -l > /opt/KUSC00402/kusc00402.txt
k get nodes
echo "2" > /opt/KUSC00402/kusc00402.txt (if the answer is 2)
cat /opt/KUSC00402/kusc00402.txt

10. Schedule a Pod as follows:
✑ Name: kucc8
✑ App Containers: 2
✑ Container Name/Images:
- nginx
- consul
Ans:
k run kucc8 --image=nginx -o yaml > 10.yaml
vi 10.yaml
apiVersion: v1
kind: Pod
metadata:
  name: kucc8
spec:
  containers:
  - name: nginx
    image: nginx
  - name: consul
    image: consul
k apply -f 10.yaml
k get nodes -o wide

11.Create a persistent volume with name app-data, of capacity 2Gi and access mode ReadOnlyMany. The type of volume is hostPath and its location
is /srv/app- data.
vi 11.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: app-data
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadOnlyMany
  hostPath:
    path: /srv/app-data

k apply -f 11.yaml
k get pv 

12. Monitor the logs of pod foo and:
✑ Extract log lines corresponding to error 9le-not-found
✑ Write them to /opt/KUTR00101/foo
Ans:
k get pods
k logs foo
k logs foo | grep "9le-not-found"
k logs foo | grep "9le-not-found" > /opt/KUTR00101/foo

13. An existing Pod needs to be integrated into the Kubernetes built-in logging architecture (e.g. kubectl logs). Adding a streaming sidecar container
is a good and common way to accomplish this requirement.
Task -
Add a sidecar container named sidecar, using the busybox image, to the existing Pod big-corp-app. The new sidecar container has to run the
following command:
/bin/sh -c "tail -n+l -f /var/log/big-corp-app.log"
Use a Volume, mounted at /var/log, to make the log 9le big-corp-app.log available to the sidecar container.
(Don't modify the specification of the existing container than adding the required volume mount.)
Ans:
vim big-app.yaml
apiVersion: v1
kind: Pod
metadata:
  name: big-corp-app
spec:
  containers:
  - name: main-container
    image: your-existing-image
    # ... existing container specifications

  - name: sidecar
    image: busybox
    command: ["/bin/sh", "-c", "tail -n +1 -f /var/log/big-corp-app.log"]
    volumeMounts:
    - name: log-volume
      mountPath: /var/log

  volumes:
  - name: log-volume
    # This assumes that the big-corp-app.log file exists in the /var/log directory
    hostPath:
      path: /var/log

k create -g big-app.yaml
k get pods




14. From the pod label name=overloaded-cpu, find pods running high CPU workloads and write the name of the pod consuming most CPU to the file
/opt/
KUTR00401/KUTR00401.txt (which already exists).
Ans:
kubectl top pod -l name=overloaded-cpu --sort-by=cpu --no-headers | awk '{print $1}' | head -n 1 > /opt/KUTR00401/KUTR00401.txt
          OR 
k top pods
k top pods -l name=overloaded-cpu --sort-by=cpu
echo "overloaded-cpu-98b9se" > /opt/KUTR00401/KUTR00401.txt
cat /opt/KUTR00401/KUTR00401.txt



15. A Kubernetes worker node, named wk8s-node-0 is in state NotReady.
Investigate why this is the case, and perform any appropriate steps to bring the node to a Ready state, ensuring that any changes are made
permanent.
Ans:
k get nodes
kubectl describe node wk8s-node-0
ssh node wk8s-node-0
sudo -i
systemctl status kubelet
systemctl enable --now kubelet
systemctl restart kubelet
exit
exit
k get nodes



16. Create a new PersistentVolumeClaim:
✑ Name: pv-volume
✑ Class: csi-hostpath-sc
✑ Capacity: 10Mi
Create a new Pod which mounts the PersistentVolumeClaim as a volume:
✑ Name: web-server
✑ Image: nginx
✑ Mount path: /usr/share/nginx/html
Configure the new Pod to have ReadWriteOnce access on the volume.
Finally, using kubectl edit or kubectl patch expand the PersistentVolumeClaim to a capacity of 70Mi and record that change.

Ans:
vim pv-claim.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pv-volume
spec:
  storageClassName: csi-hostpath-sc
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Mi

kubectl apply -f pv-claim.yaml

apiVersion: v1
kind: Pod
metadata:
  name: web-server
spec:
  containers:
  - name: nginx-container
    image: nginx
    volumeMounts:
    - name: pv-volume
      mountPath: /usr/share/nginx/html
  volumes:
  - name: pv-volume
    persistentVolumeClaim:
      claimName: pv-volume

kubectl edit pvc pv-volume

Run kubectl edit pvc pv-volume to open the PVC definition in your default editor.
Update the storage value under resources.requests to 70Mi.
OR
kubectl patch pvc pv-volume --type merge -p="{\"spec\":{\"resources\":{\"requests\":{\"storage\": \"70Mi\"}}}}"


