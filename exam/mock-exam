1. Take a backup of the etcd cluster and save it to /opt/etcd-backup.db

Ans: https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster
#ETCDCTL_API=3 etcdctl --endpoints $ENDPOINT snapshot save snapshot.db

ETCDCTL_API=3 etcdctl snapshot save -h 
cat /etckubernetes/manifest/etcd.yaml | grep file
For endpoint: vi /etckubernetes/manifest/etcd.yaml
ListenUrl=https://127.0.0.1:2379 (copy it)

ETCDCTL_API=3 etcdctl --endpoints 127.0.0.1:2379 snapshot save /opt/etcd-backup.db \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  >>snapshot saved at /opt/etcd-backup.db

---------------------------------------------------------------------------------------------------------------
2. Create a Pod called redis-storage with image redis:alpine with a volume of type emptyDir that lasts for the life of the pods.
Pod named 'redis-storage' created
Pod 'redis-storage' uses Volume type of emprtyDir
Pod 'redis-storage' uses volumeMount with mountpath=/data/redis
https://kubernetes.io/docs/concepts/storage/volumes/#emptydir-configuration-example
Ans:
k run redis-storage --image=redis:alpine --dry-run=client -o yaml > redis-storage.yaml
In yaml file, add volumes and volumeMounts
apiVersion: v1
kind: Pod
metadata:
  name: redis-storage
spec:
  containers:
  - name: redis-storage
    image: redis:alpine
    volumeMounts:
    - name: redis-storage-volume
      mountPath: /data/redis
  volumes:
  - name: redis-storage-volume
    emptyDir: {}

k apply -f redis-storage.yaml

---------------------------------------------------------------------------------------------------------------

3. Create a new pod called super-user-pod with image busybox:1.28. Allow the pod to be able to set system_time.
The container should sleep for 4800 seconds
Pod: super-user-pod
container image: busybox:1.28
SYS_TIME capabilities for the container?
Ans:
https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container
k run super-user-pod --image=busybox:1.28 --dry-run=client -o yaml >suerp-user.yaml
apiVersion: v1
kind: Pod
metadata:
  name: super-user-pod
spec:
  containers:
  - name: busybox-container
    image: busybox:1.28
    command: ["/bin/sh", "-c", "sleep 4800"]
    securityContext:
      capabilities:
        add:
        - SYS_TIME

---------------------------------------------------------------------------------------------------------------
4. A pod definition create at /root/CKA/user-pv.yaml.
Make use of this manifest file and mount the persistent volume called pv-1. 
Ensure the pod is running and the PV is bound.
mountpath: /data
persistentVolumeClaimName: my-pvc
Verify the below
persistentVolume Claim configured correctly?
Pod using the correct mountpath?
Pod using the persistent volume claim?

Ans:
cat /root/CKA/user-pv.yaml
apiVersion: v1
kind: pod
metadata:
  creationTimestamp: null
  labels:
    run: use-pv
  name: use-pv
spec:
  containers:
  - image:nginx
    name: use-pv
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

k get pv
NAME   Capacity  AccessModes
pv-1   10Mi      RWO

k get pvc
no resources 

vi pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Mi
 
k apply -f pvc.yaml
k get pvc
my-pvc  Bound   pv-1   10Mi   RWO

Edit the given pod file, using https://kubernetes.io/docs/concepts/storage/persistent-volumes/#claims-as-volumes
vi /root/CKA/user-pv.yaml
apiVersion: v1
kind: pod
metadata:
  creationTimestamp: null
  labels:
    run: use-pv
  name: use-pv
spec:
  containers:
  - image:nginx
    name: use-pv
    resources: {}
    volumeMounts:
      - mountPath: "/data"
        name: mypd
  dnsPolicy: ClusterFirst
  restartPolicy: Always
    volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: my-pvc
status: {}

k create -f /root/CKA/user-pv.yaml
k get pods
k get pods --watch
k describe pod use-pv (check pv, pvc, mountpath)
---------------------------------------------------------------------------------------------------------------
5. Create a new deployment called nginx-deploy, with image nginx:1.16 and 1 replica. 
Next upgrade the deployment to version 1.17 using rolling update.
Deployment: nginx-deployment, Image: nginx:1.16
Image: nginx:1.16
Task: Upgrade the version of the deployment to 1.17
Task: Record the changes for the image upgrade.
Ans:
k create deployment nginx-deploy --image=nginx:1.16 --replicas=1
k get deploy
k describe deployment nginx-deploy
k set image deployment/nginx-deploy nginx(this is container, so check it in describe again)=nginx:1.17
---------------------------------------------------------------------------------------------------------------

6. Create a new cluster called John. Grant him access to the cluster. 
John should have permissions to create, list, get, updaet and delete pods in the development namespace. 
The private key exists in the location: /root/CKA/john.key and csr at /root/CKA/john.csr
Important Note: As of kubernetes 1.19, the CertificateSigningRequest object expects a signerName.
Ans:
https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#create-certificatessigningrequest
CSR: 'john-developer', Status:Approved
Role Name:'developer';Namespace:'development';Resource:'Pods'
Access: User 'john' has appropriate permissions

Ans:
cat john.csr
vim john-csr.yaml

apiVersion: certificates.k8s.io/v1
kind: CertificateSigningRequest
metadata:
  name: john-developer
spec:
  request: ksdhfgruhgjgbhjbhdfgbdgjbdhfgbfhdb
  signerName: kubernetes.io/kube-apiserver-client
  usages:
  - client auth

cat john.csr | base64 | tr -d "\n"
ksdhfgruhgjgbhjbhdfgbdgjbdhfgbfhdb

vi john-csr.yaml
copy paste the above generated cert in the yaml file
k create -f john-csr.yaml 
k get csr
>>john-developer   pending(condition)
k certificate approve john-developer
k create clusterrole foo --verb=get,list,watch --resource=pods,pods/status (from help/cheatsheet)

k create clusterrole developer --verb=create,get,list,update,delete --resource=pods -n development

k get role -n development
k describe role -n development
k auth can-i get pods --namespace=development --as john
no
k auth can-i create pods --namespace=development --as john
no

k create rolebinding john-developer --role=developer --user=john -n development
k auth can-i get pods --namespace=development --as john
yes
k auth can-i create pods --namespace=development --as john
yes

---------------------------------------------------------------------------------------------------------------

7. Create a nginx pod called nginx-resolver using image nginx, expose it internally with a servie called nginx-resolver-service.
Test you are able to look up the service and pod names from within the cluster. Use the image busybox:1.28 to create a pod dns lookup. 
Record results in /root/CKA/nginx.svc and /root/CKA/nginx.pod for service and pod name resolutions respectively.
pod: nginx-resolver created
service DNS resolution recorded correctly
pod DNS resolution recorded correctly
Ans: 
k run nginx-resolver --image=nginx
k expose pod nginx-resolver --name=nginx-resolver-service --port=80
k describe svc nginx-resolver-service
k run busybox --image=busybox:1.28 -- sleep 4000
k get pods
k exec buysbox -- nslookup nginx-resolver-service > /root/CKA/nginx.svc
https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pods

k get pods -o wide (get the IP)
k exec buysbox -- nslookup 10-50-192-4.default.pod.cluster.local > /root/CKA/nginx.svc
---------------------------------------------------------------------------------------------------------------

8. Create a static pod on node01 called nginx-critical with image nginx and make sure that it is recreated/restarted automatically
in case of a failure.
Use /etc/kubernetes/manifests as the Static Pod path for example.
Ans:
k get nodes -o wide
ssh 10.7.80.3
ls /etc/kubernetes/manifests
controlplane$ k run nginx-critical --image=nginx --restart=Always --dry-run=client -o yaml
copy the yaml file paste it in node01
node01$ cat > /etc/kubernetes/manifests/nginx-critical.yaml

---------------------------------------------------------------------------------------------------------------
1. Create a new service accoutn with the name pvviewer. Grant this service account access to list all PersistentVolumes in the cluster
by creating an appropriate cluster role called pvviewer-role and ClusterRoleBinding called pvviewer-role-binding.
Next, create a pod called pvviewer with the image:redis and serviceAccount:pvviewer in the default namespace.
Ans:
k create sa pvviewer
k get sa
k clusterrole --help
k clusterrole pvviewer-role --verb=list --resources=persistentvolumes
k create clusterrolebinding --help
k create clusterrolebinding pvviewer-role-binding --clusterrole=pvviewer-role --serviceaccount=default:pvviewer

k describe clusterrolebinding pvviewer-role-binding
k run pvviewer --image=redis --dry-run=client -o yaml > pvviewer.yaml

k apply -f pvviewer.yaml

---------------------------------------------------------------------------------------------------------------
2. List the InternalIP of all nodes of the cluster. Save the result to a file /root/CKA/node_ips.
Answer should be in the format InternalIP of clusterplane <space> InternalIP of node01 
Ans:
k get nodes -o wide
k get nodes -o json | jq | grep -i internalip -B 100  --> itesms.status.addresses.type
OR
k get nodes -o json | jq -c 'paths' | grep type | grep -v condition
k get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}' | jq
k get nodes -o jsonpath='{.items[0].status.addresses[?(@.type=="InternalIP")].address}'
k get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}'

k get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}' > /root/CKA/node_ips
---------------------------------------------------------------------------------------------------------------
3. Create a pod called multi-pod with tqo containers/
Container1 name alpha, image: nginx
container2 name: beta, image:busybox, command: sleep 4000
Ans:
k run --image=nginx --dryn-client=client -o yaml >multi-pod.yaml
containers:
-  image: nginx
   name: alpha
    env:
     - name: "name"
       value: "alpha"
-  image: busybox
   name: beta
   command:
     - sleep:
     - "4000"
    env:
     - name: "name"
       value: "beta"
---------------------------------------------------------------------------------------------------------------
4. Create a pod called non-root-pod, image:redis:alpine
runAsUser: 1000
fsGroup: 2000
Ans: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
k run non-root-pod --image=redis:alpine --dry-run=client -o yaml > non-root-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: non-root-pod
  name: non-root-pod
spec:
  securityContext:
    runAsUser: 1000
    fsGroup: 2000
  volumes:
  - name: sec-ctx-vol
    emptyDir: {}
  containers:
  - name: non-root-pod
    image: redis:alpine
    resourvces: {}
  dnsPolicy: ClusterFirst
  restartpolicy: Always
status: {}

k apply -f non-root-pod.yaml
---------------------------------------------------------------------------------------------------------------
5. We have deployed a new pod called np-test-1 and a service called no-test-service. Incoming connectopns to this service are not working Troubleshoot and fix it.
Create NetworkPolicy by the name ingress-to-nptest that allows incoming connections to the service over port 80.
Ans:
k get pod
k get svc
k run curl --image=alpine/curl --rm -it -- sh
# curl np-test-service
controlplane$ k get pod --show-labels
np-test-1  run=np-test-1

vim network-policy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ingress-to-nptest
spec:
  podSelector:
    matchLabels:
      run: np-test-1
  ingress:
  - ports:
    - protocol: TCP
      port: 80
k apply -f network-policy.yaml

# curl np-test-service
---------------------------------------------------------------------------------------------------------------

6. Taint the worker node node01 to be Unschedulable. Once done, create a pod called dev-redis, image redis:alpine to ensure workloads are not
scheduled to this worker node. Finally, create a new pod called prod-redis and image: redis:alpine with toleration to be scheduled on node01.
key: env_type, value:production, operator:Equal and effect: NoSchedule
Key=env_type
Value=production
Effect=NoSchedule
prod 'dev-redis' (no tolerations) is not scheduled on node01?
Create a pod 'prod-redis' to run on node01
Ans:
k taint nodes 
kubectl taint nodes node01 key=value:NoSchedule
kubectl taint nodes node01 env_type=production:NoSchedule
k desribe node node01
k run dev-redis --image=redis:alpine
k get pods -o wide

k run prod-redis --image=redis:alpine --dry-run=client -o yaml > prod-redis.yaml
Edit the file based on this: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/#concepts
apiVersion: v1
kind: Pod
metadata:
  name: prod-redis
spec:
  containers:
  - name: redis-container
    image: redis:alpine
  tolerations:
  - key: env_type
    operator: Equal
    value: production
    effect: NoSchedule

k apply -f prod-redis.yaml

---------------------------------------------------------------------------------------------------------------
7. Create a pod called hr-pod in hr namespace belonging to the production environment and frontend tier.
Image: redis:alpine
Use appropriate labels and create all the required objects if it doesn't not exist in the system already.
hr-pod labeled with environment production?
hr-pod labeled with the frontend?
Ans:
k get ns 
k create ns hr
k run hr-pod -n hr --image=redis:alpine --labels="environment=production,tier=frontend"
pod/hr-pod created
k get pods -n hr
---------------------------------------------------------------------------------------------------------------
8. A kubeconfig file called super.kubeconfig has been created under /root/CKA. There is something wrong with the configuration. 
Troubleshoot and fix it.
Ans:
k get nodes --kubeconfig /root/CKA/super.kubeconfig
error: "https://controlplane:9999/api?timeout=32s"

cat .kube/config
default port is 6443
vi /root/CKA/super.kubeconfig
update the code with 6443 port

k get nodes --kubeconfig /root/CKA/super.kubeconfig
---------------------------------------------------------------------------------------------------------------
9. We have created a new deployment called nginx-deploy. Scale the deployment to 3 replicas.
Has the replicas increased? Troubleshoot the issue and fix it.
Ans:
k get deployments
k scale deployment nginx-ddeploy --replicas=3
k describe deployment
3 desired but Message says, scaled up replica set nginx-deply-678 to 1

So, there's an issue here only taking replica as 1

S0, need to check with controller manager.
k get pods -n kube-system
kube-controller-manager-controlplane   ImagePullBackOff

cd /etc/kubernetes/manifests
vim kube-controller-manager.yaml
spelling mistakes: controller

k get pods -n kube-system

k get deployments
------------------------------------------------------------------------------------------
Q: ETCD Key Value store which is running as a pod in cluster1 . 
Take the backup of it and store it on the cluster1-controlplane node at the path /opt/cluster1_backup.db.
You can ssh to the controlplane node by running ssh root@cluster1-controlplane from the student-node.

NOTE: - If the etcd utility tool is unavailable on the controlplane, install it first.
Ans:

ssh root@cluster1-controlplane
cd /tmp
export RELEASE=$(curl -s https://api.github.com/repos/etcd-io/etcd/releases/latest | grep tag_name | cut -d '"' -f 4)
wget https://github.com/etcd-io/etcd/releases/download/v3.5.0/etcd-v3.5.0-linux-amd64.tar.gz
tar xzvf etcd-v3.5.0-linux-amd64.tar.gz
sudo mv etcd-v3.5.0-linux-amd64/etcdctl /usr/local/bin/
ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key snapshot save /opt/cluster1_backup.db

------------------------------------------------------------------------------------------
Q: Create a generic secret called secure-sec-cka12-arch in the secure-sys-cka12-arch namespace on the cluster3. 
Use the key/value of color=darkblue to create the secret.
Ans: kubectl create ns secure-sys-cka12-arch --context cluster3
kubectl create secret generic secure-sec-cka12-arch --from-literal=color=darkblue -n secure-sys-cka12-arch --context cluster3

------------------------------------------------------------------------------------------
Q: Create a service account called deploy-cka20-arch. 
Further create a cluster role called deploy-role-cka20-arch with permissions to get the deployments in cluster1.
Finally create a cluster role binding called deploy-role-binding-cka20-arch to bind deploy-role-cka20-arch cluster role with deploy-cka20-arch service account.

Ans:
kubectl --context cluster1 create serviceaccount deploy-cka20-arch
kubectl --context cluster1 create clusterrole deploy-role-cka20-arch --resource=deployments --verb=get
kubectl --context cluster1 create clusterrolebinding deploy-role-binding-cka20-arch --clusterrole=deploy-role-cka20-arch --serviceaccount=default:deploy-cka20-arch

verify:
kubectl --context cluster1 auth can-i get deployments --as=system:serviceaccount:default:deploy-cka20-arch

------------------------------------------------------------------------------------------
Q: kubectl config use-context cluster1

One of the nginx based pod called cyan-pod-cka28-trb is running under cyan-ns-cka28-trb namespace and it is exposed within the cluster using cyan-svc-cka28-trb service.
This is a restricted pod so a network policy called cyan-np-cka28-trb has been created in the same namespace to apply some restrictions on this pod.
Two other pods called cyan-white-cka28-trb and cyan-black-cka28-trb are also running in the default namespace.
The nginx based app running on the cyan-pod-cka28-trb pod is exposed internally on the default nginx port (80).
Expectation: This app should only be accessible from the cyan-white-cka28-trb pod.
Problem: This app is not accessible from anywhere.
Troubleshoot this issue and fix the connectivity as per the requirement listed above.
Note: You can exec into cyan-white-cka28-trb and cyan-black-cka28-trb pods and test connectivity using the curl utility.
You may update the network policy, but make sure it is not deleted from the cyan-ns-cka28-trb namespace.

Ans:
Let's look into the network policy

kubectl edit networkpolicy cyan-np-cka28-trb -n cyan-ns-cka28-trb
Under spec: -> egress: you will notice there is not cidr: block has been added, 
since there is no restrcitions on egress traffic so we can update it as below. 
Further you will notice that the port used in the policy is 8080 but 
the app is running on default port which is 80 so let's update this as well (under egress and ingress):

Change port: 8080 to port: 80
- ports:
  - port: 80
    protocol: TCP
  to:
  - ipBlock:
      cidr: 0.0.0.0/0
Now, lastly notice that there is no POD selector has been used in ingress section but this app is supposed 
to be accessible from cyan-white-cka28-trb pod under default namespace. 
So let's edit it to look like as below:

ingress:
- from:
  - namespaceSelector:
      matchLabels:
        kubernetes.io/metadata.name: default
   podSelector:
      matchLabels:
        app: cyan-white-cka28-trb
Now, let's try to access the app from cyan-white-pod-cka28-trb

kubectl exec -it cyan-white-cka28-trb -- sh
curl cyan-svc-cka28-trb.cyan-ns-cka28-trb.svc.cluster.local
Also make sure its not accessible from the other pod(s)

kubectl exec -it cyan-black-cka28-trb -- sh
curl cyan-svc-cka28-trb.cyan-ns-cka28-trb.svc.cluster.local
It should not work from this pod. So its looking good now.
-------------------------------------------------------------------------------------------------------
Q: There is a deployment called nodeapp-dp-cka08-trb created in the default namespace on cluster1. 
This app is using an ingress resource named nodeapp-ing-cka08-trb.
From cluster1-controlplane host we should be able to access this app using the command: curl http://kodekloud-ingress.app. 
However, it is not working at the moment. Troubleshoot and fix the issue.
Note: You should be able to ssh into the cluster1-controlplane using ssh cluster1-controlplane command.

Ans:
SSh into cluster1-controlplane
ssh cluster1-controlplane
Try to access the app using curl http://kodekloud-ingress.app command. You will see 404 Not Found error.

Look into the ingress to make sure its configued properly.

kubectl get ingress
kubectl edit ingress nodeapp-ing-cka08-trb
Under rules: -> host: change example.com to kodekloud-ingress.app
Under backend: -> service: -> name: Change example-service to nodeapp-svc-cka08-trb
Change port: -> number: from 80 to 3000
You should be able to access the app using curl http://kodekloud-ingress.app command now.
-------------------------------------------------------------------------------------------------------
Q: kubectl config use-context cluster1
The purple-app-cka27-trb pod is an nginx based app on the container port 80. This app is exposed within the cluster using a ClusterIP type service called purple-svc-cka27-trb.
There is another pod called purple-curl-cka27-trb which continuously monitors the status of the app running within purple-app-cka27-trb pod by accessing the purple-svc-cka27-trb service using curl.
Recently we started seeing some errors in the logs of the purple-curl-cka27-trb pod.
Dig into the logs to identify the issue and make sure it is resolved.
Note: You will not be able to access this app directly from the student-node but you can exec into the purple-app-cka27-trb pod to check.
Ans:

kubectl logs purple-curl-cka27-trb
You will see some logs as below

Not able to connect to the nginx app on http://purple-svc-cka27-trb
Now to debug let's try to access this app from within the purple-app-cka27-trb pod

kubectl exec -it purple-app-cka27-trb -- bash
curl http://purple-svc-cka27-trb
exit
You will notice its stuck, so app is not reachable. Let's look into the service to see its configured correctly.

kubectl edit svc purple-svc-cka27-trb
Under ports: -> port: and targetPort: is set to 8080 but nginx default port is 80 so change 8080 to 80 and save the changes
Let's check the logs now

kubectl logs purple-curl-cka27-trb
You will see Thank you for using nginx. in the output now.
-------------------------------------------------------------------------------------------------------
Q: There is a Cronjob called orange-cron-cka10-trb which is supposed to run every two minutes (i.e 13:02, 13:04, 13:06…14:02, 14:04…and so on). This cron targets the application running inside the orange-app-cka10-trb pod to make sure the app is accessible. 
The application has been exposed internally as a ClusterIP service.
However, this cron is not running as per the expected schedule and is not running as intended.
Make the appropriate changes so that the cronjob runs as per the required schedule and it passes the accessibility checks every-time.
Ans:
kubectl get cronjob (Check the cron schedule)
Make sure the schedule for orange-cron-cka10-trb crontjob is set to */2 * * * * if not then edit it.

Also before that look for the issues why this cron is failing

kubectl logs orange-cron-cka10-trb-xxxx
You will see some error like

curl: (6) Could not resolve host: orange-app-cka10-trb
You will notice that the curl is trying to hit orange-app-cka10-trb directly but it is supposed to hit the relevant service which is orange-svc-cka10-trb so we need to fix the curl command.

Edit the cronjob
kubectl edit cronjob orange-cron-cka10-trb
Change schedule * * * * * to */2 * * * *
Change command curl orange-app-cka10-trb to curl orange-svc-cka10-trb
Wait for 2 minutes to run again this cron and it should complete now.
-------------------------------------------------------------------------------------------------------
Q:kubectl config use-context cluster4
There is some issue on the student-node preventing it from accessing the cluster4 Kubernetes Cluster.
Troubleshoot and fix this issue. Make sure that you are able to run the kubectl commands (For example: kubectl get node --context=cluster4) from the student-node.
The kubeconfig for all the clusters is stored in the default kubeconfig file: /root/.kube/config on the student-node.
Ans:
vim /root/.kube/config
update the port from 64433 to 6443 under cluster4
kubectl get node --context=cluster4
-------------------------------------------------------------------------------------------------------
Q: kubectl config use-context cluster2
The yello-cka20-trb pod is stuck in a Pending state. Fix this issue and get it to a running state. Recreate the pod if necessary.
Do not remove any of the existing taints that are set on the cluster nodes.
Ans:
kubectl get pod --context=cluster2
So you will see that yello-cka20-trb pod is in Pending state. Let's check out the relevant events.

kubectl get event --field-selector involvedObject.name=yello-cka20-trb --context=cluster2
You will see some errors like:

Warning   FailedScheduling   pod/yello-cka20-trb   0/2 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 1 node(s) had untolerated taint {node: node01}. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.
Notice this error 1 node(s) had untolerated taint {node: node01} so we can see that one of nodes have taints applied. We don't want to remove the node taints and we are not going to re-create the POD so let's look into the POD config if its using any other toleration settings.

kubectl get pod yello-cka20-trb --context=cluster2 -o yaml
You will notice this in the output

tolerations:
  - effect: NoSchedule
    key: node
    operator: Equal
    value: cluster2-node01
Here notice that the value for key node is cluster2-node01 but the node has different value applied i.e node01 so let's update the taints values for the node as needed.

kubectl --context=cluster2 taint nodes cluster2-node01 node=cluster2-node01:NoSchedule --overwrite=true
Let's check the POD status again

kubectl get pod --context=cluster2
It should be in Running state now.
-------------------------------------------------------------------------------------------------------
Q: kubectl config use-context cluster1

Create a new deployment called ocean-tv-wl09 in the default namespace using the image kodekloud/webapp-color:v1.
Use the following specs for the deployment:
1. Replica count should be 3.
2. Set the Max Unavailable to 40% and Max Surge to 55%.
3. Create the deployment and ensure all the pods are ready.
4. After successful deployment, upgrade the deployment image to kodekloud/webapp-color:v2 and inspect the deployment rollout status.
5. Check the rolling history of the deployment and on the student-node, save the current revision count number to the /opt/revision-count.txt file.
6. Finally, perform a rollback and revert back the deployment image to the older version.
Ans:
kubectl config use-context cluster1

Use the following template to create a deployment called ocean-tv-wl09: -

---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: ocean-tv-wl09
  name: ocean-tv-wl09
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ocean-tv-wl09
  strategy: 
   type: RollingUpdate
   rollingUpdate:
     maxUnavailable: 40%
     maxSurge: 55%
  template:
    metadata:
      labels:
        app: ocean-tv-wl09
    spec:
      containers:
      - image: kodekloud/webapp-color:v1
        name: webapp-color


Now, create the deployment by using the kubectl create -f command in the default namespace: -

kubectl create -f <FILE-NAME>.yaml
After sometime, upgrade the deployment image to kodekloud/webapp-color:v2: -
kubectl set image deploy ocean-tv-wl09 webapp-color=kodekloud/webapp-color:v2
And check out the rollout history of the deployment ocean-tv-wl09: -

kubectl rollout history deploy ocean-tv-wl09
deployment.apps/ocean-tv-wl09 
REVISION  CHANGE-CAUSE
1         <none>
2         <none>


NOTE: - Revision count is 2. In your lab, it could be different.
On the student-node, store the revision count to the given file: -
echo "2" > /opt/revision-count.txt
In final task, rollback the deployment image to an old version: -
kubectl rollout undo deployment ocean-tv-wl09
Verify the image name by using the following command: -
kubectl describe deploy ocean-tv-wl09
It should be kodekloud/webapp-color:v1 image.
-------------------------------------------------------------------------------------------------------
Q: kubectl config use-context cluster3
One of our applications runs on the cluster3-controlplane node. Due to the possibility of traffic increase, we want to scale the application pods to loadbalance the traffic and provide a smoother user experience.
cluster3-controlplane node has enough resources to deploy more application pods. Scale the deployment called essports-wl02 to 5.
Ans:
kubectl config use-context cluster3
Now, get the details of the nodes: -
kubectl get nodes -o wide
then SSH to the given node by the following command: -
ssh cluster3-controlplane
And run the kubectl scale command as follows: -
kubectl scale deploy essports-wl02 --replicas=5
OR
You can run the kubectl scale command from the student node as well: -
kubectl scale deploy essports-wl02 --replicas=5
Verify the scaled-up pods by kubectl get command: -
kubectl get pods
The number of pods should be 1 to 5.
-------------------------------------------------------------------------------------------------------
Q: kubectl config use-context cluster3
One of our Junior DevOps engineers have deployed a pod nginx-wl06 on the cluster3-controlplane node. However, while specifying the resource limits, instead of using Mebibyte as the unit, Gebibyte was used.
As a result, the node doesn't have sufficient resources to deploy this pod and it is stuck in a pending state
Fix the units and re-deploy the pod (Delete and recreate the pod if needed).
Ans: 
kubectl config use-context cluster3
Run the following command to check the pending pods on all the namespaces: -
kubectl get pods -A
After that, inspect the pod Events as follows: -
kubectl get pods -A | grep -i pending
kubectl describe po nginx-wl06
Make use of the kubectl edit command to update the values from Gi to Mi:-
kubectl edit po nginx-wl06
It will save the temporary file under the /tmp/ directory. Use the kubectl replace command as follows: -
kubectl replace -f /tmp/kubectl-edit-xxxx.yaml --force
It will delete the existing pod and will re-create it again with new changes.
-------------------------------------------------------------------------------------------------------
Q:kubectl config use-context cluster1
Create a persistent volume called data-pv-cka02-str with the below properties:
- Its capacity should be 128Mi.
- The volume type should be hostpath and path should be /opt/data-pv-cka02-str.
Next, create a persistent volume claim called data-pvc-cka02-str as per below properties:
- Request 50Mi of storage from data-pv-cka02-str PV.
Ans:
Set context to cluster1:

Create a yaml template as below:
apiVersion: v1
kind: PersistentVolume
metadata:
  name: data-pv-cka02-str
spec:
  capacity:
    storage: 128Mi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /opt/data-pv-cka02-str
  storageClassName: manual

---

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: data-pvc-cka02-str
spec:
  storageClassName: manual
  volumeName: data-pv-cka02-str
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Mi
Apply the template:
kubectl apply -f <template-file-name>.yaml
-------------------------------------------------------------------------------------------------------
Q: kubectl config use-context cluster3
There is a deployment nginx-deployment-cka04-svcn in cluster3 which is exposed using service nginx-service-cka04-svcn.
Create an ingress resource nginx-ingress-cka04-svcn to load balance the incoming traffic with the following specifications:
pathType: Prefix and path: /
Backend Service Name: nginx-service-cka04-svcn
Backend Service Port: 80
ssl-redirect is set to false
Ans:
First change the context to "cluster3":
student-node ~ ➜  kubectl config use-context cluster3
Switched to context "cluster3".
Now apply the ingress resource with the given requirements:

kubectl apply -f - << EOF
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nginx-ingress-cka04-svcn
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
  - http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: nginx-service-cka04-svcn
            port:
              number: 80
EOF


Check if the ingress resource was successfully created:

student-node ~ ➜  kubectl get ingress
NAME                       CLASS    HOSTS   ADDRESS       PORTS   AGE
nginx-ingress-cka04-svcn   <none>   *       172.25.0.10   80      13s

As the ingress controller is exposed on cluster3-controlplane using traefik service, 
we need to ssh to cluster3-controlplane first to check if the ingress resource works properly:

student-node ~ ➜  ssh cluster3-controlplane

cluster3-controlplane:~# curl -I 172.25.0.11
HTTP/1.1 200 OK
...
-------------------------------------------------------------------------------------------------------
Q: kubectl config use-context cluster3

Part I:
Create a ClusterIP service .i.e. service-3421-svcn in the spectra-1267 ns which should expose the pods namely pod-23 and pod-21 with port set to 8080 and targetport to 80.

Part II:
Store the pod names and their ip addresses from the spectra-1267 ns at /root/pod_ips_cka05_svcn where the output is sorted by their IP's.

Please ensure the format as shown below:
POD_NAME        IP_ADDR
pod-1           ip-1
pod-3           ip-2
pod-2           ip-3
...
-------------------------------------------------------------------------------------------------------
Q: A template to create a Kubernetes pod is stored at /root/red-probe-cka12-trb.yaml on the student-node. However, using this template as-is is resulting in an error.
Fix the issue with this template and use it to create the pod. Once created, watch the pod for a minute or two to make sure its stable i.e, it's not crashing or restarting.
Make sure you do not update the args: section of the template.
Ans:
Erros shows after apply the template,
error: error validating "red-probe-cka12-trb.yaml": error validating data: [ValidationError(Pod.spec.containers[0].livenessProbe.httpGet): unknown field "command" in io.k8s.api.core.v1.HTTPGetAction, 
ValidationError(Pod.spec.containers[0].livenessProbe.httpGet): missing required field "port" in io.k8s.api.core.v1.HTTPGetAction]; if you choose to ignore these errors, turn validation off with --validate=false

Under livenessProbe: you will see the type is httpGet however the rest of the options are command based so this probe should be of exec type.
Change httpGet to exec
POD status(logs/events): 
kubectl get event --field-selector involvedObject.name=red-probe-cka12-trb
Notice the command - sleep 3 ; touch /healthcheck; sleep 30;sleep 30000 it starts with a delay of 3 seconds, but the liveness probe initialDelaySeconds is set to 1 and failureThreshold is also 1. 
Which means the POD will fail just after first attempt of liveness check which will happen just after 1 second of pod start. So to make it stable we must increase the initialDelaySeconds to at least 5
Change initialDelaySeconds from 1 to 5 and save apply the changes.
kubectl delete pod red-probe-cka12-trb
kubectl apply -f red-probe-cka12-trb.yaml
----------------------------------------------------------------------------------------------------------------
Q: kubectl config use-context cluster1
Q: There is a script located at /root/pod-cka26-arch.sh on the student-node. 
Update this script to add a command to filter/display the label with value component of the pod called kube-apiserver-cluster1-controlplane (on cluster1) using jsonpath.
Ans: 
kubectl --context cluster1 get pod -n kube-system kube-apiserver-cluster1-controlplane -o jsonpath=‘{.metadata.labels.component}’
-------------------------------------------------------------------------------------------------------
Q: Find the pod that consumes the most memory and store the result to the file /opt/high_memory_pod in the following format cluster_name,namespace,pod_name.
The pod could be in any namespace in any of the clusters that are currently configured on the student-node.
NOTE: It's recommended to wait for a few minutes to allow deployed objects to become fully operational and start consuming resources.
Ans:
kubectl top pods -A --context cluster1 --no-headers | sort -nr -k3 | head -1
kube-system       kube-apiserver-cluster1-controlplane            48m   262Mi   
kubectl top pods -A --context cluster2 --no-headers | sort -nr -k3 | head -1
kube-system   kube-apiserver-cluster2-controlplane            44m   258Mi   
kubectl top pods -A --context cluster3 --no-headers | sort -nr -k3 | head -1
default       backend-cka06-arch                        205m   596Mi   
kubectl top pods -A --context cluster4 --no-headers | sort -nr -k3 | head -1
kube-system   kube-apiserver-cluster4-controlplane            43m   266Mi   

echo cluster3,default,backend-cka06-arch > /opt/high_memory_pod
------------------------------------------------------------------------------------------------------------------------------------------
Q: Find the pod that consumes the most CPU and store the result to the file /opt/high_cpu_pod in the following format cluster_name,namespace,pod_name.
The pod could be in any namespace in any of the clusters that are currently configured on the student-node.
NOTE: It's recommended to wait for a few minutes to allow deployed objects to become fully operational and start consuming resources.
Ans: 
student-node ~ ➜  kubectl top pods -A --context cluster1 --no-headers | sort -nr -k3 | head -1
student-node ~ ➜  kubectl top pods -A --context cluster2 --no-headers | sort -nr -k3 | head -1
student-node ~ ➜  kubectl top pods -A --context cluster3 --no-headers | sort -nr -k3 | head -1
student-node ~ ➜  kubectl top pods -A --context cluster4 --no-headers | sort -nr -k3 | head -1
Using this, find the pod that uses most CPU. In this case, it is kube-apiserver-cluster1-controlplane on cluster1.
Save the result in the correct format to the file:
student-node ~ ➜  echo cluster1,kube-system,kube-apiserver-cluster1-controlplane > /opt/high_cpu_pod
------------------------------------------------------------------------------------------------------------------------------------------
Q: Find the node across all clusters that consumes the most memory and store the result to the file /opt/high_memory_node in the following format cluster_name,node_name
Ans:
kubectl top node --context cluster1 --no-headers | sort -nr -k2 | head -1
kubectl top node --context cluster2 --no-headers | sort -nr -k2 | head -1
kubectl top node --context cluster3 --no-headers | sort -nr -k2 | head -1
kubectl top node --context cluster4 --no-headers | sort -nr -k2 | head -1
echo cluster3,cluster3-controlplane > /opt/high_memory_node
------------------------------------------------------------------------------------------------------------------------------------------
Q: There is a deployment called nodeapp-dp-cka08-trb created in the default namespace on cluster1. 
This app is using an ingress resource named nodeapp-ing-cka08-trb.
From cluster1-controlplane host we should be able to access this app using 
the command: curl http://kodekloud-ingress.app. However, it is not working at the moment. Troubleshoot and fix the issue.
Ans:
ingress.yaml is as below and might need the corrections as below.
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  creationTimestamp: "2024-03-08T17:24:07Z"
  generation: 1
  name: nodeapp-ing-cka08-trb
  namespace: default
  resourceVersion: "6277"
  uid: 258fe6d9-7de4-4dc3-ba1a-d2d1c5982b77
spec:
  ingressClassName: nginx
  rules:
  - host: kodekloud-ingress.app   <--update here
    http:
      paths:
      - backend:
          service:
            name: nodeapp-svc-cka08-trb   #<-- update here
            port:
              number: 3000    <-- 80 to 3000
        path: /
        pathType: Prefix
status:
  loadBalancer:
    ingress:
    - ip: 192.32.19.17
------------------------------------------------------------------------------------------------------------------------------------------
Q: There is a pod called pink-pod-cka16-trb created in the default namespace in cluster4. 
This app runs on port tcp/5000 and it is exposed to end-users using an ingress resource called pink-ing-cka16-trb 
in such a way that it is supposed to be accessible using the command: curl http://kodekloud-pink.app on cluster4-controlplane host.
However, this is not working. Troubleshoot and fix this issue, making any necessary to the objects.
Note: You should be able to ssh into the cluster4-controlplane using ssh cluster4-controlplane command.
Ans:
ssh cluster4-controlplane
curl kodekloud-pink.app (503 error)
kubectl edit svc pink-svc-cka16-trb
Under ports: change protocol: UDP to protocol: TCP
curl kodekloud-pink.app (curl: (6) Could not resolve host: example.com)
kubectl get deploy -n kube-system
>>You will see that for coredns all relicas are down, you will see 0/0 ready pods. So let's scale up this deployment.)
kubectl scale --replicas=2 deployment coredns -n kube-system
curl kodekloud-pink.app

------------------------------------------------------------------------------------------------------------------------------------------
Q:The cat-cka22-trb pod is stuck in Pending state. Look into the issue to fix the same. 
Make sure that the pod is in running state and its stable (i.e not restarting or crashing).
Note: Do not make any changes to the pod (No changes to pod config but you may destory and re-create).
Ans:
kubectl get pod (pending state)
kubectl --context cluster2 get event --field-selector involvedObject.name=cat-cka22-trb
logs:
Warning   FailedScheduling   pod/cat-cka22-trb   0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 2 node(s) didn't match Pod's node affinity/selector. 
preemption: 0/2 nodes are available: 3 Preemption is not helpful for scheduling.

So seems like this POD is using the node affinity, let's look into the POD to understand the node affinity its using.
kubectl --context cluster2 get pod cat-cka22-trb -o yaml
Under affinity: you will see its looking for key: node and values: cluster2-node02 so let's verify if node01 has these labels applied.
kubectl --context cluster2 get node cluster2-node01 -o yaml
Look under labels: and you will not find any such label, so let's add this label to this node.

kubectl label node cluster2-node01 node=cluster2-node01
kubectl get node cluster2-node01 -o yaml
kubectl --context cluster2 get pod
kubectl --context cluster2 logs -f cat-cka22-trb
The HOST variable seems incorrect, it must be set to kodekloud(log)
kubectl --context cluster2 get pod -o yaml
env:
- name: HOST
  valueFrom:
    secretKeyRef:
      key: hostname
      name: cat-cka22-trb
So we can see that HOST variable is defined and its value is being retrieved from a secret called "cat-cka22-trb". Let's look into this secret.
kubectl --context cluster2 get secret
kubectl --context cluster2 get secret cat-cka22-trb -o yaml
echo "<the decoded value you see for hostname" | base64 -d (You will find a key/value pair under data:, let's try to decode it to see its value:)
echo "kodekloud" | base64
kubectl edit secret cat-cka22-trb
Change requests storage hostname: a29kZWtsb3Vkdg== to hostname: a29kZWtsb3VkCg== (values may vary)
POD should be good now.

------------------------------------------------------------------------------------------------------------------------------------------
Q:In the dev-wl07 namespace, one of the developers has performed a rolling update and upgraded the application to a newer version. 
But somehow, application pods are not being created.
To get back the working state, rollback the application to the previous version .
After rolling the deployment back, on the controlplane node, save the image currently in use to the /root/rolling-back-record.txt file and 
increase the replica count to the 5.
You can SSH into the cluster1 using ssh cluster1-controlplane command.
Ans:
kubectl config use-context cluster1
kubectl get pods -n dev-wl07
kubectl rollout undo -n dev-wl07 deploy webapp-wl07
kubectl describe deploy -n dev-wl07 webapp-wl07 | grep -i image
>>Image:        kodekloud/webapp-color
ssh cluster1-controlplane
echo "kodekloud/webapp-color" > /root/rolling-back-record.txt
kubectl scale deploy -n dev-wl07 webapp-wl07 --replicas=5
kubectl get deploy -n dev-wl07

------------------------------------------------------------------------------------------------------------------------------------------
Q: demo-pod-cka29-trb pod is stuck in aPending state, look into issue to fix the same, Make sure pod is in Running state and stable.
Ans:
kubectl get event --field-selector involvedObject.name=demo-pod-cka29-trb
Warning:
Warning   FailedScheduling   pod/demo-pod-cka29-trb   0/3 nodes are available: 3 pod has unbound immediate PersistentVolumeClaims. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.
kubectl get pvc
kubectl get event --field-selector involvedObject.name=demo-pvc-cka29-trb
Error as:
Warning   VolumeMismatch   persistentvolumeclaim/demo-pvc-cka29-trb   Cannot bind to requested volume "demo-pv-cka29-trb": incompatible accessMode
Edit the PVC and PV
kubectl get pvc demo-pvc-cka29-trb -o yaml > /tmp/pvc.yaml
vi /tmp/pvc.yaml

kubectl replace --force -f /tmp/pvc.yaml
OR
kubectl delete pvc demo-pvc-cka29-trb
kubectl apply -f /tmp/pvc.yaml 

kubectl get pod demo-pod-cka29-trb

------------------------------------------------------------------------------------------------------------------------------------------
Q: One of our applications runs on the cluster3-controlplane node. Due to the possibility of traffic increase, we want to scale the application pods to loadbalance the traffic and provide a smoother user experience.
cluster3-controlplane node has enough resources to deploy more application pods. Scale the deployment called essports-wl02 to 5.
Ans:
kubectl config use-context cluster3
kubectl get nodes -owide
ssh cluster3-controlplane
k scale deploy essports-wl02 --replicas=5
------------------------------------------------------------------------------------------------------------------------------------------
Q:
A persistent volume called papaya-pv-cka09-str is already created with a storage capacity of 150Mi. 
It's using the papaya-stc-cka09-str storage class with the path /opt/papaya-stc-cka09-str.
Also, a persistent volume claim named papaya-pvc-cka09-str has also been created on this cluster. 
This PVC has requested 50Mi of storage from papaya-pv-cka09-str volume.
Resize the PVC to 80Mi and make sure the PVC is in Bound state.
Ans:
Edit papaya-pv-cka09-str PV:

kubectl get pv papaya-pv-cka09-str -o yaml > /tmp/papaya-pv-cka09-str.yaml
Edit the template:

vi /tmp/papaya-pv-cka09-str.yaml
Delete all entries for uid:, annotations, status:, claimRef: from the template.
Edit papaya-pvc-cka09-str PVC:

kubectl get pvc papaya-pvc-cka09-str -o yaml > /tmp/papaya-pvc-cka09-str.yaml
Edit the template:

vi /tmp/papaya-pvc-cka09-str.yaml
Under resources: -> requests: change storage: 50Mi to storage: 80Mi and save the template.
Delete the exsiting PVC:

kubectl delete pvc papaya-pvc-cka09-str
Delete the exsiting PV and create using the template:

kubectl delete pv papaya-pv-cka09-str
kubectl apply -f /tmp/papaya-pv-cka09-str.yaml
Create the PVC using template:

kubectl apply -f /tmp/papaya-pvc-cka09-str.yaml

/tmp/papaya-pv-cka09-str.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"PersistentVolume","metadata":{"annotations":{},"name":"papaya-pv-cka09-str"},"spec":{"accessModes":["ReadWriteOnce"],"capacity":{"storage":"150Mi"},"local":{"path":"/opt/papaya-stc-cka09-str"},"nodeAffinity":{"required":{"nodeSelectorTerms":[{"matchExpressions":[{"key":"kubernetes.io/hostname","operator":"In","values":["cluster1-controlplane"]}]}]}},"persistentVolumeReclaimPolicy":"Retain","storageClassName":"papaya-stc-cka09-str"}}
    pv.kubernetes.io/bound-by-controller: "yes"
  creationTimestamp: "2024-03-11T21:39:28Z"
  finalizers:
  - kubernetes.io/pv-protection
  name: papaya-pv-cka09-str
  resourceVersion: "13387"
  uid: f050d40d-5179-4d87-87b0-8d8670c96e1b
spec:
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 150Mi
  claimRef:
    apiVersion: v1
       kind: PersistentVolumeClaim
    name: papaya-pvc-cka09-str
    namespace: default
    resourceVersion: "13385"
  local:
    path: /opt/papaya-stc-cka09-str
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - cluster1-controlplane
  persistentVolumeReclaimPolicy: Retain
  storageClassName: papaya-stc-cka09-str
  volumeMode: Filesystem
status:
  phase: Bound
------------------------------------------------------------------------------------------------------------------------------------------
Q: A pod called color-app-cka13-arch has been created in the default namespace. This pod logs can be accessed using kubectl logs -f color-app-cka13-arch command from the student-node. 
It is currently displaying Color is pink output. Update the pod definition file to make use of the environment variable with the value - green and recreate this pod.
Ans: k get pod color-app-cka13-arch -o yaml > green.yaml
k delete pod color-app-cka13-arch
k logs -f color-app-cka13-arch
>>color is green
color is green

------------------------------------------------------------------------------------------------------------------------------------------
Q: For this question, please set the context to cluster1 by running:
kubectl config use-context cluster1
There is an existing persistent volume called orange-pv-cka13-trb. A persistent volume claim called orange-pvc-cka13-trb is created to claim storage from orange-pv-cka13-trb.
However, this PVC is stuck in a Pending state. As of now, there is no data in the volume.
Troubleshoot and fix this issue, making sure that orange-pvc-cka13-trb PVC is in Bound state.
Ans:
Describe the PVC and determine the issue

kubectl describe pvc orange-pvc-cka13-trb
Note the message "requested PV is too small". We must adjust the PVC to fit

Describe the PV and determine its properties. Note that PVC properties must be adjusted to match

kubectl describe pv orange-pv-cka13-trb
Adjust the PVC. Note that you cannot directly edit a PVC size to be smaller, so we have to replace it.

kubectl get pvc orange-pvc-cka13-trb -o yaml  > pvc.yaml
vi pvc.yaml
Change the requested size to match the size of the PV. Save and exit vi, then replace the PVC with the edited manifest:

kubectl replace --force -f pvc.yaml
------------------------------------------------------------------------------------------------------------------------------------------
Q: The blue-dp-cka09-trb deployment is having 0 out of 1 pods running. Fix the issue to make sure that pod is up and running.
Ans:
k get pod
NAME                               READY   STATUS                  RESTARTS      AGE
backend-cka06-arch                 1/1     Running                 0             51m
blue-dp-cka09-trb-cb7678c8-cftmx   0/1     Init:CrashLoopBackOff   3 (17s ago)   65s

k logs blue-dp-cka09-trb-cb7678c8-cftmx -c init-container

k edit deploy blue-dp-cka09-trb
   initContainers:
   - command:
     - sh
     - -c
     - echo 'Welcome!'
k get event --field-selector involvedObject.name=blue-dp-cka09-trb-cb7678c8-cftmx

k edit deploy blue-dp-cka09-trb
------------------------------------------------------------------------------------------------------------------------------------------
Q: For this question, please set the context to cluster1 by running:

kubectl config use-context cluster1
We want to deploy a python based application on the cluster using a template located at /root/olive-app-cka10-str.yaml on student-node. However, before you proceed we need to make some modifications to the YAML file as per details given below:

The YAML should also contain a persistent volume claim with name olive-pvc-cka10-str to claim a 100Mi of storage from olive-pv-cka10-str PV.
Update the deployment to add a sidecar container, which can use busybox image (you might need to add a sleep command for this container to keep it running.)
Share the python-data volume with this container and mount the same at path /usr/src. Make sure this container only has read permissions on this volume.
Finally, create a pod using this YAML and make sure the POD is in Running state.
Missing from the question, but required to pass
Create a nodeport service for this deployment with the following specification
Node port: 32006
Name: olive-svc-cka10-str
Ans:
Solution
Examine what we have...

cat /root/olive-app-cka10-str
The PVC volume claim is already present. Look for the PVC

kubectl get pvc
It is not present, therefore it will have to be created first.

kubectl get pv
The PV exists. Note the ACCESS MODES and STORAGECLASS, which are required in the PVC manifest, along with the storage request given in the question.

Prepare manfiest for the new PVC

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: olive-pvc-cka10-str
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 100Mi
  storageClassName: olive-stc-cka10-str
Then create it. It will not bind yet until the pod is created.

Adjust the pod as directed, and add the service to the end.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: olive-app-cka10-str
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: olive-app-cka10-str
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/hostname
                operator: In
                values:
                  - cluster1-node01
      containers:
      - name: busybox
        image: busybox
        command:        # <- Any variation of sleep command should work.
        - bin/sh        # Needs to sleep long enough to get to end of test.
        - -c
        - sleep 10000
        volumeMounts:
        - mountPath: /usr/src
          name: python-data
          readOnly: true
      - name: python
        image: poroko/flask-demo-app
        ports:
        - containerPort: 5000
        volumeMounts:
        - name: python-data
          mountPath: /usr/share/
      volumes:
      - name: python-data
        persistentVolumeClaim:
          claimName: olive-pvc-cka10-str
  selector:
    matchLabels:
      app: olive-app-cka10-str

---

apiVersion: v1
kind: Service
metadata:
  name: olive-svc-cka10-str
  namespace: default
spec:
  ports:
  - nodePort: 32006
    port: 5000
    protocol: TCP
    targetPort: 5000
  selector:
    app: olive-app-cka10-str
  type: NodePort

Create the resources

kubectl apply -f /root/olive-app-cka10-str
------------------------------------------------------------------------------------------------------------------------------------------
Q: For this question, please set the context to cluster3 by running:
kubectl config use-context cluster3
A manifest file is available at the /root/app-wl03/ on the student-node node.
There are some issues with the file; hence couldn't deploy a pod on the cluster3-controlplane node.
After fixing the issues, deploy the pod, and it should be in a running state.
NOTE: - Ensure that the existing limits are unchanged.
Ans:
kubectl config use-context cluster3
cd /root/app-wl03/
>> app-wl03.yaml
k create -f app-wl03.yaml
>> The Pod "app-wl03" is invalid: spec.containers[0].resources.requests: Invalid value: "1Gi": must be less than or equal to memory limit
vi app-wl03.yaml
resources:
     requests:
       memory: 100Mi
     limits:
       memory: 100Mi

kubectl create -f app-wl03.yaml 
pod/app-wl03 created
 k get pods
NAME                        READY   STATUS    RESTARTS   AGE
alpine-sleeper-cka15-arch   1/1     Running   0          110m
app-wl03                    1/1     Running   0          13s
------------------------------------------------------------------------------------------------------------------------------------------
Q: There is a persistent volume named apple-pv-cka04-str. Create a persistent volume claim named apple-pvc-cka04-str and request a 40Mi of storage from apple-pv-cka04-str PV.
The access mode should be ReadWriteOnce and storage class should be manual.
Ans:
vi apple-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: apple-pvc-cka04-str
spec:
  volumeName: apple-pv-cka04-str
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 40Mi

k apply -f apple-pvc.yaml

------------------------------------------------------------------------------------------------------------------------------------------
Q: For this question, please set the context to cluster3 by running:
kubectl config use-context cluster3
Create a deployment named hr-web-app-cka08-svcn using the image kodekloud/webapp-color with 2 replicas.
Expose the hr-web-app-cka08-svcn as service hr-web-app-service-cka08-svcn application on port 30082 on the nodes of the cluster.
The web application listens on port 8080.
Ans:
kubectl config use-context cluster3
kubectl create deployment  hr-web-app-cka08-svcn --image=kodekloud/webapp-color --replicas=2
kubectl expose deployment hr-web-app-cka08-svcn --type=NodePort --port=8080 --name=hr-web-app-service-cka08-svcn --dry-run=client -o yaml > hr-web-app-service-cka08-svcn.yaml

Now, in generated service definition file add the nodePort field with the given port number under the ports section and create a service.
hr-web-app-service-cka08-svcn.yaml
nodePort: 30082
--------------------------------------------------------------------------------------------------------------------------------------------
Q: For this question, please set the context to cluster3 by running:
kubectl config use-context cluster3

Part I:
Create a ClusterIP service .i.e. service-3421-svcn in the spectra-1267 ns which should expose the pods namely pod-23 and pod-21 
with port set to 8080 and targetport to 80.

Part II:
Store the pod names and their ip addresses from the spectra-1267 ns at /root/pod_ips_cka05_svcn where the output is sorted by their IP's.

Please ensure the format as shown below:
POD_NAME        IP_ADDR
pod-1           ip-1
pod-3           ip-2
pod-2           ip-3
...

Ans:
kubectl config use-context cluster3
The easiest way to route traffic to a specific pod is by the use of labels and selectors . List the pods along with their labels:
student-node ~ ➜  kubectl get pods --show-labels -n spectra-1267
NAME     READY   STATUS    RESTARTS   AGE     LABELS
pod-12   1/1     Running   0          5m21s   env=dev,mode=standard,type=external
pod-34   1/1     Running   0          5m20s   env=dev,mode=standard,type=internal
pod-43   1/1     Running   0          5m20s   env=prod,mode=exam,type=internal
pod-23   1/1     Running   0          5m21s   env=dev,mode=exam,type=external
pod-32   1/1     Running   0          5m20s   env=prod,mode=standard,type=internal
pod-21   1/1     Running   0          5m20s   env=prod,mode=exam,type=external
Looks like there are a lot of pods created to confuse us. But we are only concerned with the labels of pod-23 and pod-21.
As we can see both the required pods have labels mode=exam,type=external in common. Let's confirm that using kubectl too:
student-node ~ ➜  kubectl get pod -l mode=exam,type=external -n spectra-1267                                    
NAME     READY   STATUS    RESTARTS   AGE
pod-23   1/1     Running   0          9m18s
pod-21   1/1     Running   0          9m17s
Nice!! Now as we have figured out the labels, we can proceed further with the creation of the service:
kubectl create service clusterip service-3421-svcn -n spectra-1267 --tcp=8080:80 --dry-run=client -o yaml > service-3421-svcn.yaml
Now modify the service definition with selectors as required before applying to k8s cluster:
student-node ~ ➜  cat service-3421-svcn.yaml 
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: service-3421-svcn
  name: service-3421-svcn
  namespace: spectra-1267
spec:
  ports:
  - name: 8080-80
    port: 8080
    protocol: TCP
    targetPort: 80
  selector:
    app: service-3421-svcn  # delete 
    mode: exam    # add
    type: external  # add
  type: ClusterIP
status:
  loadBalancer: {}

Finally let's apply the service definition:
kubectl apply -f service-3421-svcn.yaml
service/service-3421 created

k get ep service-3421-svcn -n spectra-1267
NAME           ENDPOINTS                     AGE
service-3421   10.42.0.15:80,10.42.0.17:80   52s
To store all the pod name along with their IP's , we could use imperative command as shown below:

kubectl get pods -n spectra-1267 -o=custom-columns='POD_NAME:metadata.name,IP_ADDR:status.podIP' --sort-by=.status.podIP
POD_NAME   IP_ADDR
pod-12     10.42.0.18
pod-23     10.42.0.19
pod-34     10.42.0.20
pod-21     10.42.0.21
...
store the output to /root/pod_ips
kubectl get pods -n spectra-1267 -o=custom-columns='POD_NAME:metadata.name,IP_ADDR:status.podIP' --sort-by=.status.podIP > /root/pod_ips_cka05_svcn
------------------------------------------------------------------------------------------------------------------------------------------
Q:
kubectl config use-context cluster3
Create a ReplicaSet with name checker-cka10-svcn in ns-12345-svcn namespace with image registry.k8s.io/e2e-test-images/jessie-dnsutils:1.3.
Make sure to specify the below specs as well:
command sleep 3600
replicas set to 2
container name: dns-image
Once the checker pods are up and running, store the output of the command nslookup kubernetes.default from any one of the checker pod into the file /root/dns-output-12345-cka10-svcn on student-node.
Ans:
kubectl config use-context cluster3
Create the ReplicaSet as per the requirements:



kubectl apply -f - << EOF
---
apiVersion: v1
kind: Namespace
metadata:
  creationTimestamp: null
  name: ns-12345-svcn
spec: {}
status: {}

---
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: checker-cka10-svcn
  namespace: ns-12345-svcn
  labels:
    app: dns
    tier: testing
spec:
  replicas: 2
  selector:
    matchLabels:
      tier: testing
  template:
    metadata:
      labels:
        tier: testing
    spec:
      containers:
      - name: dns-image
        image: registry.k8s.io/e2e-test-images/jessie-dnsutils:1.3
        command:
          - sleep
          - "3600"
EOF

k get pods -n ns-12345-svcn 
NAME                       READY   STATUS    RESTARTS   AGE
checker-cka10-svcn-d2cd2   1/1     Running   0          12s
checker-cka10-svcn-qj8rc   1/1     Running   0          12s

POD_NAME=`k get pods -n ns-12345-svcn --no-headers | head -1 | awk '{print $1}'`

kubectl exec -n ns-12345-svcn -i -t $POD_NAME -- nslookup kubernetes.default
>> ;; connection timed out; no servers could be reached
command terminated with exit code 1
There seems to be a problem with the name resolution. Let's check if our coredns pods are up and if any service exists to reach them:
k get pods -n kube-system | grep coredns
k get svc -n kube-system 
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   62m
Everything looks okay here but the name resolution problem exists, let's see if the kube-dns service have any active endpoints:

kubectl get ep -n kube-system kube-dns 
NAME       ENDPOINTS   AGE
kube-dns   <none>      63m

Finally, we have our culprit.
If we dig a little deeper, we will it is using wrong labels and selector:
kubectl describe svc -n kube-system kube-dns 
Name:              kube-dns
Namespace:         kube-system
....
Selector:          k8s-app=core-dns
Type:              ClusterIP

kubectl get deploy -n kube-system --show-labels | grep coredns
coredns   2/2     2            2           66m   k8s-app=kube-dns

kubectl patch service -n kube-system kube-dns -p '{"spec":{"selector":{"k8s-app": "kube-dns"}}}'
service/kube-dns patched

kubectl get ep -n kube-system kube-dns 
NAME       ENDPOINTS                                              AGE
kube-dns   10.50.0.2:53,10.50.192.1:53,10.50.0.2:53 + 3 more...   69m

NOTE: We can use any method to update kube-dns service. In our case, we have used kubectl patch command.

Now let's store the correct output to /root/dns-output-12345-cka10-svcn:

kubectl exec -n ns-12345-svcn -i -t $POD_NAME -- nslookup kubernetes.default
Server:         10.96.0.10
Address:        10.96.0.10#53

Name:   kubernetes.default.svc.cluster.local
Address: 10.96.0.1

kubectl exec -n ns-12345-svcn -i -t $POD_NAME -- nslookup kubernetes.default > /root/dns-output-12345-cka10-svcn
------------------------------------------------------------------------------------------------------------------------------------------
Q: 
kubectl config use-context cluster3
Create a loadbalancer service with name wear-service-cka09-svcn to expose the deployment webapp-wear-cka09-svcn application in app-space namespace.
Ans:
kubectl config use-context cluster3

kubectl expose -n app-space deployment webapp-wear-cka09-svcn --type=LoadBalancer --name=wear-service-cka09-svcn --port=8080
service/wear-service-cka09-svcn exposed

k get svc -n app-space
NAME                      TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
wear-service-cka09-svcn   LoadBalancer   10.43.68.233   172.25.0.14   8080:32109/TCP   14s
------------------------------------------------------------------------------------------------------------------------------------------
Q: A pod called elastic-app-cka02-arch is running in the default namespace. The YAML file for this pod is available at /root/elastic-app-cka02-arch.yaml on the student-node. 
The single application container in this pod writes logs to the file /var/log/elastic-app.log.
One of our logging mechanisms needs to read these logs to send them to an upstream logging server but we don't want to increase the read overhead for our main application container 
so recreate this POD with an additional sidecar container that will run along with the application container and print to the STDOUT by running the command tail -f /var/log/elastic-app.log. 
You can use busybox image for this sidecar container.
Given yaml is below,
apiVersion: v1
kind: Pod
metadata:
  name: elastic-app-cka02-arch
spec:
  containers:
  - name: elastic-app
    image: busybox:1.28
    args:
    - /bin/sh
    - -c
    - >
      mkdir /var/log;
      i=0;
      while true;
      do
        echo "$(date) INFO $i" >> /var/log/elastic-app.log;
        i=$((i+1));
        sleep 1;
      done
    volumeMounts:
    - name: varlog
      mountPath: /var/log
    - name: sidecar     <--- added from here to 
      image: busybox:1.28
      args: [/bin/sh, -c, 'tail -f  /var/log/elastic-app.log']
      volumeMounts:
    - name: varlog
      mountPath: /var/log   <--- till here added
    volumes:
    - name: varlog
      emptyDir: {}

------------------------------------------------------------------------------------------------------------------------------------------
Q: We have created a service account called green-sa-cka22-arch, a cluster role called green-role-cka22-arch and a cluster role binding called green-role-binding-cka22-arch.
Update the permissions of this service account so that it can only get all the namespaces in cluster1.
kubectl edit clusterrole green-role-cka22-arch --context cluster1
At the end add below code:
- apiGroups:
  - "*"
  resources:
  - namespaces
  verbs:
  - get
You can verify it as below:
kubectl auth can-i get namespaces --as=system:serviceaccount:default:green-sa-cka22-arch
yes
------------------------------------------------------------------------------------------------------------------------------------------
Q: The db-deployment-cka05-trb deployment is having 0 out of 1 PODs ready.
Figure out the issues and fix the same but make sure that you do not remove any DB related environment variables from the deployment/pod.
Ans:
kubectl get pod
kubectl logs <pod-name>
>>Error from server (BadRequest): container "db" in pod "db-deployment-cka05-trb-7457c469b7-zbvx6" is waiting to start: CreateContainerConfigError
kubectl get event --field-selector involvedObject.name=<pod-name>
>>Error: couldn't find key db in Secret default/db-cka05-trb  (Error)
kubectl get secrets db-root-pass-cka05-trb -o yaml
kubectl get secrets db-user-pass-cka05-trb -o yaml
kubectl get secrets db-cka05-trb -o yaml
kubectl edit deployment db-deployment-cka05-trb -o yaml
You will notice that some of the keys are different what are reffered in the deployment.

Change some env keys: db to database , db-user to username and db-password to password
Change a secret reference: db-user-cka05-trb to db-user-pass-cka05-trb
Finally save the changes.
------------------------------------------------------------------------------------------------------------------------------------------
Q: We tried to schedule grey-cka21-trb pod on cluster4 which was supposed to be deployed by the kubernetes scheduler so far but somehow its stuck in Pending state. 
Look into the issue and fix the same, make sure the pod is in Running state.
You can SSH into the cluster4 using ssh cluster4-controlplane command.
Ans:
kubectl get pod --context=cluster4
kubectl logs grey-cka21-trb --context=cluster4
kubectl get event --context=cluster4 --field-selector involvedObject.name=grey-cka21-trb
kubectl get pod --context=cluster4 -n kube-system
>> must be in crashing state
kubectl logs kube-scheduler-cluster4-controlplane --context=cluster4 -n kube-system
>> run.go:74] "command failed" err="failed to get delegated authentication kubeconfig: failed to get delegated authentication 
kubeconfig: stat /etc/kubernetes/scheduler.config: no such file or directory"
ssh cluster4-controlplane
ls /etc/kubernetes/scheduler.config
>> You won't find it, instead the correct file is /etc/kubernetes/scheduler.conf so let's modify the manifest.
Search for config in the file, you will find some typos, change every occurence of /etc/kubernetes/scheduler.config to /etc/kubernetes/scheduler.conf.
kubectl get pod -A
------------------------------------------------------------------------------------------------------------------------------------------
Q: We have deployed a simple web application called frontend-wl04 on cluster1. This version of the application has some issues from 
a security point of view and needs to be updated to version 2.
Update the image and wait for the application to fully deploy.
You can verify the running application using the curl command on the terminal:
student-node ~ ➜  curl http://cluster1-node01:30080
<!doctype html>
<title>Hello from Flask</title>
<body style="background: #2980b9;"></body>
<div style="color: #e4e4e4;
    text-align:  center;
    height: 90px;
    vertical-align:  middle;">
  <h1>Hello from frontend-wl04-84fc69bd96-p7rbl!</h1>
  <h2>
    Application Version: v1
  </h2>
</div>
student-node ~ ➜ 
Version 2 Image details as follows:
1. Current version of the image is `v1`, we need to update with the image to kodekloud/webapp-color:v2.
2. Use the imperative command to update the image.
Ans:
•	Set the context: -
kubectl config use-context cluster1

Now, test the current version of the application as follows:
student-node ~ ➜  curl http://cluster1-node01:30080
<!doctype html>
<title>Hello from Flask</title>
<body style="background: #2980b9;"></body>
<div style="color: #e4e4e4;
    text-align:  center;
    height: 90px;
    vertical-align:  middle;">
  <h1>Hello from frontend-wl04-84fc69bd96-p7rbl!</h1>
  <h2>
    Application Version: v1
  </h2>
</div>
student-node ~ ➜

Let's update the image, First, run the below command to check the existing image: -
kubectl get deploy frontend-wl04 -o yaml | grep -i image 

After checking the existing image, we have to use the imperative command (It will take less than a minute) to update the image: -
kubectl set image deploy frontend-wl04 simple-webapp=kodekloud/webapp-color:v2

Finally, run the below command to check the updated image: -
kubectl get deploy frontend-wl04 -o yaml | grep -i image 

It should be the kodekloud/webapp-color:v2 image and the same should be visible when you run the curl command again:
student-node ~ ➜  curl http://cluster1-node01:30080
<!doctype html>
<title>Hello from Flask</title>
<body style="background: #16a085;"></body>
<div style="color: #e4e4e4;
    text-align:  center;
    height: 90px;
    vertical-align:  middle;">
  <h1>Hello from frontend-wl04-6c54f479df-5tddd!</h1>
  <h2>
    Application Version: v2
  </h2>
</div>
------------------------------------------------------------------------------------------------------------------------------------------
Q: The deployment called trace-wl08 inside the test-wl08 namespace on cluster1 has undergone several, routine, rolling updates and rollbacks.
Inspect the revision 2 of this deployment and store the image name that was used in this revision 
in the /opt/trace-wl08-revision-book.txt file on the student-node.
Ans: kubectl config use-context cluster1

kubectl get deploy -n test-wl08
kubectl rollout history deployment -n test-wl08 trace-wl08 
kubectl rollout history deployment -n test-wl08 trace-wl08 --revision=2
Under the Containers section, you will see the image name.
On the student-node, save that image name in the given file /opt/trace-wl08-revision-book.txt:
echo "busybox:1.35" > /opt/trace-wl08-revision-book.txt
cat /opt/trace-wl08-revision-book.txt
------------------------------------------------------------------------------------------------------------------------------------------
Q: Create a pod with name tester-cka02-svcn in dev-cka02-svcn namespace with image registry.k8s.io/e2e-test-images/jessie-dnsutils:1.3. 
Make sure to use command sleep 3600 with restart policy set to Always .
Once the tester-cka02-svcn pod is running, store the output of the command nslookup kubernetes.default from tester 
pod into the file /root/dns_output on student-node.
Ans:
kubectl config use-context cluster1
kubectl create ns dev-cka02-svcn
Create the pod as per the requirement
kubectl apply -f - << EOF
apiVersion: v1
kind: Pod
metadata:
  name: tester-cka02-svcn
  namespace: dev-cka02-svcn
spec:
  containers:
  - name: tester-cka02-svcn
    image: registry.k8s.io/e2e-test-images/jessie-dnsutils:1.3
    command:
      - sleep
      - "3600"
  restartPolicy: Always
EOF

student-node ~ ➜  kubectl exec -n dev-cka02-svcn -i -t tester-cka02-svcn -- nslookup kubernetes.default
;; connection timed out; no servers could be reached
command terminated with exit code 1

Looks like something is broken at the moment, if we observe the kube-system namespace, we will see no coredns pods are not running which is 
creating the problem, let's scale them for the nslookup command to work:

kubectl scale deployment -n kube-system coredns --replicas=2
kubectl exec -n dev-cka02-svcn -i -t tester-cka02-svcn -- nslookup kubernetes.default >> /root/dns_output
student-node ~ ➜  cat /root/dns_output
Server:         10.96.0.10
Address:        10.96.0.10#53

Name:   kubernetes.default.svc.cluster.local
Address: 10.96.0.1
------------------------------------------------------------------------------------------------------------------------------------------
Q: Create a nginx pod called nginx-resolver-cka06-svcn using image nginx, expose it internally with a service called nginx-resolver-service-cka06-svcn.
Test that you are able to look up the service and pod names from within the cluster. Use the image: busybox:1.28 for dns lookup. 
Record results in /root/CKA/nginx.svc.cka06.svcn and /root/CKA/nginx.pod.cka06.svcn
Ans:
kubectl config use-context cluster1

To create a pod nginx-resolver-cka06-svcn and expose it internally:
kubectl run nginx-resolver-cka06-svcn --image=nginx 
kubectl expose pod/nginx-resolver-cka06-svcn --name=nginx-resolver-service-cka06-svcn --port=80 --target-port=80 --type=ClusterIP 

To create a pod test-nslookup. Test that you are able to look up the service and pod names from within the cluster:
kubectl run test-nslookup --image=busybox:1.28 --rm -it --restart=Never -- nslookup nginx-resolver-service-cka06-svcn
kubectl run test-nslookup --image=busybox:1.28 --rm -it --restart=Never -- nslookup nginx-resolver-service-cka06-svcn > /root/CKA/nginx.svc.cka06.svcn

Get the IP of the nginx-resolver-cka06-svcn pod and replace the dots(.) with hyphon(-) which will be used below.
student-node ~ ➜  kubectl get pod nginx-resolver-cka06-svcn -o wide
IP=`kubectl get pod nginx-resolver-cka06-svcn -o wide --no-headers | awk '{print $6}' | tr '.' '-'`
kubectl run test-nslookup --image=busybox:1.28 --rm -it --restart=Never -- nslookup $IP.default.pod > /root/CKA/nginx.pod.cka06.svcn
------------------------------------------------------------------------------------------------------------------------------------------
Q: A pod named beta-pod-cka01-arch has been created in the beta-cka01-arch namespace. Inspect the logs and save all logs starting with the string 
ERROR in file /root/beta-pod-cka01-arch_errors on the student-node.
Ans: 
kubectl -n beta-cka01-arch logs beta-pod-cka01-arch --context cluster1 | grep ERROR > /root/beta-pod-cka01-arch_errors
head /root/beta-pod-cka01-arch_errors

------------------------------------------------------------------------------------------------------------------------------------------
Q: The controlplane node called cluster4-controlplane in the cluster4 cluster is planned for a regular maintenance. In preparation for this maintenance work, 
we need to take backups of this cluster. However, something is broken at the moment!
Troubleshoot the issues and take a snapshot of the ETCD database using the etcdctl utility at the location /opt/etcd-boot-cka18-trb.db.
Note: Make sure etcd listens at its default port. Also you can SSH to the cluster4-controlplane host using the ssh cluster4-controlplane command from the student-node.
Ans:
ssh cluster4-controlplane
ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key snapshot save /opt/etcd-boot-cka18-trb.db
>>It might stuck for forever, let's see why that would happen. Try to list the PODs first
kubectl get pod -A
>>The connection to the server cluster4-controlplane:6443 was refused - did you specify the right host or port?
journalctl -u kubelet -f
You will see a lot of connect: connection refused erros but that must be because the different cluster components are not able to connect to the api server 
so try to filter out these logs to look more closely
journalctl -u kubelet -f | grep -v 'connect: connection refused'
>>cluster4-controlplane kubelet[2240]: E0923 04:38:15.630925    2240 file.go:187] "Could not process manifest file" err="invalid pod: [spec.containers[0].volumeMounts[1].name: Not found: \"etcd-cert\"]" path="/etc/kubernetes/manifests/etcd.yaml"
vi /etc/kubernetes/manifests/etcd.yaml 
Search for etcd-cert, you will notice that the volume name is etcd-certs but the volume mount is trying to mount etcd-cert volume 
which is incorrect. Fix the volume mount name and save the changes. Let's restart kubelet service after that.
systemctl restart kubelet
kubectl get pod -A
ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key snapshot save /opt/etcd-boot-cka18-trb.db

------------------------------------------------------------------------------------------------------------------------------------------
Q: A storage class called coconut-stc-cka01-str was created earlier.
Use this storage class to create a persistent volume called coconut-pv-cka01-str as per below requirements:
- Capacity should be 100Mi.
- The volume type should be hostpath and the path should be /opt/coconut-stc-cka01-str.
- Use coconut-stc-cka01-str storage class.
- This volume must be created on cluster1-node01 (the /opt/coconut-stc-cka01-str directory already exists on this node).
- It must have a label with key: storage-tier with value: gold.
Also create a persistent volume claim with the name coconut-pvc-cka01-str as per below specs:
- Request 50Mi of storage from coconut-pv-cka01-str PV, it must use matchLabels to use the PV.
- Use coconut-stc-cka01-str storage class.
- The access mode must be ReadWriteMany.
Ans:
pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: coconut-pv-cka01-str
  labels:
    storage-tier: gold
spec:
  capacity:
    storage: 100Mi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: coconut-stc-cka01-str
  hostPath:
    path: /opt/coconut-stc-cka01-str
    type: DirectoryOrCreate
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - cluster1-node01

pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: coconut-pvc-cka01-str
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: coconut-stc-cka01-str
  resources:
    requests:
      storage: 50Mi
  selector:
    matchLabels:
      storage-tier: gold

------------------------------------------------------------------------------------------------------------------------------------------
Q: There is a deployment called nginx-dp-cka04-trb which has been used to deploy a static website. 
The access to this website can be tested by running: curl http://kodekloud-exam.app:30002. 
However, it is not working at the moment.
Troubleshoot and fix it.
Ans: 
kubectl get pod
kubectl logs -f <pod-name>
kubectl get event --field-selector involvedObject.name=<pod-name>
Error as below:
70s         Warning   FailedMount   pod/nginx-dp-cka04-trb-767b767dc-6c5wk   Unable to attach or mount volumes: unmounted volumes=[nginx-config-volume-cka04-trb], 
unattached volumes=[index-volume-cka04-trb kube-api-access-4fbrb nginx-config-volume-cka04-trb]: timed out waiting for the condition
kubectl get deploy nginx-dp-cka04-trb -o=yaml
Under volumes: look for the configMap: name which is nginx-configuration-cka04-trb. Now lets look into this configmap.
kubectl get configmap nginx-configuration-cka04-trb
The above command will fail as there is no configmap with this name, so now list the all configmaps.
kubectl get configmap
You will see an configmap named nginx-config-cka04-trb which seems to be the correct one.
kubectl edit deploy nginx-dp-cka04-trb
Under configMap: change nginx-configuration-cka04-trb to nginx-config-cka04-trb. Once done wait for the POD to come up.
curl http://kodekloud-exam.app:30002

------------------------------------------------------------------------------------------------------------------------------------------
Q: Create a persistent volume called red-pv-cka03-str of type: hostPath type, use the path /opt/red-pv-cka03-str and capacity: 100Mi.
Ans:
apiVersion: v1
kind: PersistentVolume
metadata:
  name: red-pv-cka03-str
spec:
  capacity:
    storage: 100Mi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /opt/red-pv-cka03-str

------------------------------------------------------------------------------------------------------------------------------------------
Q: kubectl config use-context cluster1
John is setting up a two tier application stack that is supposed to be accessible using the service curlme-cka01-svcn. 
To test that the service is accessible, he is using a pod called curlpod-cka01-svcn. 
However, at the moment, he is unable to get any response from the application.
Troubleshoot and fix this issue so the application stack is accessible.
While you may delete and recreate the service curlme-cka01-svcn, please do not alter it in anyway.
Ans:
Test if the service curlme-cka01-svcn is accessible from pod curlpod-cka01-svcn or not.
kubectl exec curlpod-cka01-svcn -- curl curlme-cka01-svcn
We did not get any response. Check if the service is properly configured or not.
kubectl describe svc curlme-cka01-svcn ''
The service has no endpoints configured. As we can delete the resource, 
let's delete the service and create the service again.
kubectl delete svc curlme-cka01-svcn
kubectl expose pod curlme-cka01-svcn --port=80
OR
apiVersion: v1
kind: Service
metadata:
  labels:
    run: curlme-cka01-svcn
  name: curlme-cka01-svcn
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    run: curlme-cka01-svcn
  type: ClusterIP

  kubectl exec curlpod-cka01-svcn -- curl curlme-cka01-svcn
------------------------------------------------------------------------------------------------------------------------------------------
Q:A service account called deploy-cka19-trb is created in cluster1 along with a cluster role called deploy-cka19-trb-role. This role should have the permissions to get all the deployments under the default namespace. However, at the moment, it is not able to. 
Find out what is wrong and correct it so that the deploy-cka19-trb service account is able to get deployments under default namespace.
Let's see if deploy-cka19-trb service account is able to get the deployments.
kubectl auth can-i get deployments --as=system:serviceaccount:default:deploy-cka19-trb
We can see its not since we are getting no in the output.
Let's look into the cluster role:
kubectl get clusterrole deploy-cka19-trb-role -o yaml
The rules would be fine but we can see that there is no cluster role binding and service account associated with this. So let's create a cluster role binding.
kubectl create clusterrolebinding deploy-cka19-trb-role-binding --clusterrole=deploy-cka19-trb-role --serviceaccount=default:deploy-cka19-trb
Let's see if deploy-cka19-trb service account is able to get the deployments now.

kubectl auth can-i get deployments --as=system:serviceaccount:default:deploy-cka19-trb

------------------------------------------------------------------------------------------------------------------------------------------
Q: We deployed an app using a deployment called web-dp-cka06-trb. it's using the httpd:latest image. 
There is a corresponding service called web-service-cka06-trb that exposes this app on the node port 30005. 
However, the app is not accessible!
Troubleshoot and fix this issue. Make sure you are able to access the app using curl http://kodekloud-exam.app:30005 command.
Ans:
kubectl get deploy
kubectl get pod
kubectl get event --field-selector involvedObject.name=web-dp-cka06-trb-
kubectl get pvc
>>You should see web-pvc-cka06-trb in the output but as per logs the POD was looking for web-cka06-trb PVC. 
Let's update the deployment to fix this.
kubectl edit deploy web-dp-cka06-trb
>>Under volumes: -> name: web-str-cka06-trb -> persistentVolumeClaim: -> claimName change web-cka06-trb to web-pvc-cka06-trb and save the changes.
Look into the POD again to make sure its running now
kubectl get pod
kubectl edit deploy web-dp-cka06-trb
>>Under spec: -> containers: -> change image from httpd:letest to httpd:latest and save the changes
kubectl get pod
kubectl logs web-dp-cka06-trb-
kubectl get event --field-selector involvedObject.name=web-dp-cka06-trb-xxxx --sort-by='.lastTimestamp'
kubectl edit deploy web-dp-cka06-trb
>> •	Under containers: -> lifecycle: -> postStart: -> exec: -> command: change /bin to /bin/sh
kubectl get pod
curl http://kodekloud-exam.app:30005
kubectl edit svc web-service-cka06-trb
>> You will note that service is using selector: -> app: web-cka06-trb
kubectl get deploy web-dp-cka06-trb -o yaml
>> Under labels you will see labels: -> deploy: web-app-cka06-trb
kubectl edit svc web-service-cka06-trb

------------------------------------------------------------------------------------------------------------------------------------------
Q: We recently deployed a DaemonSet called logs-cka26-trb under kube-system namespace in cluster2 for collecting logs from all the cluster nodes including the controlplane node. 
However, at this moment, the DaemonSet is not creating any pod on the controlplane node.
Troubleshoot the issue and fix it to make sure the pods are getting created on all nodes including the controlplane node.
Ans: 
kubectl --context cluster2 get ds logs-cka26-trb -n kube-system
kubectl --context cluster2 get pod  -n kube-system
kubectl --context cluster2 get pod logs-cka26-trb-8czd9 -n kube-system -o wide
kubectl --context cluster2 edit ds logs-cka26-trb -n kube-system
Under tolerations: add below given tolerations as well
- key: node-role.kubernetes.io/control-plane
  operator: Exists
  effect: NoSchedule

------------------------------------------------------------------------------------------------------------------------------------------
Q:There is a requirement to share a volume between two containers that are running within the same pod. 
Use the following instructions to create the pod and related objects: 
- Create a pod named grape-pod-cka06-str.
- The main container should use the nginx image and mount a volume
called grape-vol-cka06-str at path /var/log/nginx.
- The sidecar container can use busybox image, 
you might need to add a sleep command to this container to keep it running. Next, mount the same volume called grape-vol-cka06-str at the path /usr/src.
The volume should be of type emptyDir.
Ans:

------------------------------------------------------------------------------------------------------------------------------------------